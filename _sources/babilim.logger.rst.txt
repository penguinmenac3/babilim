Logging & Checkpoints
=====================

This is important for you if you want to find your checkpoints and tensorboard logs.

Using it is super easy, if you defined your experiment configuration correct.
Simply pass your config to the setup function.

.. code-block:: python

    import babilim.logger as logger
    # Create all folder structures for logging and init logging or use an existing checkpoint.
    logger.setup(config, continue_training=False)

The continue_training and continue_with_specific_checkpointpath allow you to continue a training that was aborted for some reason.
Did your annoying colleague press the power button again? No problem, just use on of these two options.


**Autogenerated Documentation**

.. automodule:: babilim.logger
   :members:
   :undoc-members:
   :show-inheritance:

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# babilim.model.module\n",
    "\n",
    "> The base class for every module.\n",
    "\n",
    "This code is under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2019 Michael Fuerst\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Sequence, Any, Sequence, Callable, Dict, Iterable\n",
    "from collections import defaultdict\n",
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import babilim\n",
    "from babilim import PYTORCH_BACKEND, TF_BACKEND, warn\n",
    "from babilim.core import StatefullObject, RunOnlyOnce, Tensor\n",
    "from babilim.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "# Module\n",
    "\n",
    "A module is the base building block of all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Module(StatefullObject):\n",
    "    def __init__(self, layer_type: str):\n",
    "        \"\"\"\n",
    "        A module is the base building block of all models and layers.\n",
    "\n",
    "        Users must overwrite `__init__`,`call` and `build`. For the latter do not forget the `@RunOnlyOnce`-Annotation.\n",
    "        The rest comes pre implemented and should not be overwritten.\n",
    "        When overwriting the `__init__` call the super init: `super().__init__(layer_type)`.\n",
    "\n",
    "        :param layer_type: The type of the layer provided.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.initialized_module = False\n",
    "        self.__layer_type = layer_type\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Makes a module callable. Automatically wraps tensorflow or pytorch tensors to ITensors from babilim.\n",
    "\n",
    "        <code>module = MyModule()</code>\n",
    "\n",
    "        <code>module(*args, **kwargs)</code>\n",
    "\n",
    "        Warning: This function should not be overwritten. Instead overwrite `call` with no underscores.\n",
    "\n",
    "        :param *args: All indexed parameters of your call function derivate.\n",
    "        :param **kwargs: All named parameters of your call function derivate.\n",
    "        \"\"\"\n",
    "        # ensure that call gets called with ITensor objects but the caller can use native tensors.\n",
    "        args, wrapped_args = self._wrapper.wrap(args)\n",
    "        kwargs, wrapped_kwargs = self._wrapper.wrap(kwargs)\n",
    "        self.build(*args, **kwargs)\n",
    "        result = self.call(*args, **kwargs)\n",
    "        parent_dict = inspect.stack()[1][0].f_locals\n",
    "        if \"self\" in parent_dict:\n",
    "            parent = parent_dict[\"self\"]\n",
    "            self._register_params(parent)\n",
    "        if wrapped_args or wrapped_kwargs:\n",
    "            return self._wrapper.unwrap(result)\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def initialize(self, dataset: Dataset):\n",
    "        \"\"\"\n",
    "        Initializes your module by running a sample of your dataset through it.\n",
    "\n",
    "        :param dataset: The dataset you want to use for initialization. (Must be of type babilim.data.Dataset)\n",
    "        \"\"\"\n",
    "        if not self.initialized_module:\n",
    "            if babilim.DEBUG_VERBOSITY:\n",
    "                babilim.info(\"Build Model\")\n",
    "            self.initialized_module = True\n",
    "            dataloader = dataset.to_dataloader()\n",
    "            features, _ = next(iter(dataloader))\n",
    "            self(**features._asdict())\n",
    "\n",
    "    def predict(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Pass in single training examples as numpy arrays.\n",
    "        And predict the value without gradients.\n",
    "        Should be used for testing and evaluation.\n",
    "\n",
    "        If your network has eval modes you need to set them manually.\n",
    "\n",
    "        The array must not have batch dimension.\n",
    "\n",
    "        :param kwargs: The parameters to feed the network as a single example.\n",
    "        :return: The output for a single example.\n",
    "        \"\"\"\n",
    "        kwargs = {k: np.array([kwargs[k]]) for k in kwargs.keys() if isinstance(kwargs[k], np.ndarray)}\n",
    "        kwargs = {k: Tensor(data=kwargs[k], trainable=False) for k in kwargs.keys()}\n",
    "\n",
    "        preds = self.__call__(**kwargs)\n",
    "        tmp = preds._asdict()\n",
    "        tmp = {k: tmp[k].numpy()[0] for k in tmp.keys()}\n",
    "        preds = type(preds)(**tmp)\n",
    "        return preds\n",
    "\n",
    "    def build(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        This function will build your model and must be annotated with the RunOnlyOnce-Annotation.\n",
    "\n",
    "        Allocating weight tensors should be done here.\n",
    "        You can make use of the knowledge of your inputs to compute shapes for your weight tensors.\n",
    "        This will make coding dimensions a lot easier.\n",
    "\n",
    "        <code> @RunOnlyOnce</code>\n",
    "\n",
    "        <code>def build(self, image: ITensor) -> None:</code>\n",
    "\n",
    "        :param *args: You must specify the exact same parameters as for your call.\n",
    "        :param **kwargs: You must specify the exact same parameters as for your call.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def call(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Makes a module callable and contains the forward pass of your model.\n",
    "        This should be pure computation and not allocate any weights.\n",
    "        Allocating weights should be done in the `build` function.\n",
    "\n",
    "        This function gets called by `__call__` and must be overwritten by any derived class.\n",
    "\n",
    "        <code>def call(self, image: ITensor) -> NetworkOutput:</code>\n",
    "\n",
    "        :param *args: You can specify any parameters you want.\n",
    "        :param **kwargs: You can specify any named parameters you want.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Every modules must implement this method.\")\n",
    "\n",
    "    @property\n",
    "    def layer_type(self):\n",
    "        return self.__layer_type\n",
    "\n",
    "    @property\n",
    "    def submodules(self):\n",
    "        modules = []\n",
    "        for k in self.__dict__:\n",
    "            v = self.__dict__[k]\n",
    "            if isinstance(v, Module):\n",
    "                modules.append(v)\n",
    "                modules.append(v.submodules)\n",
    "        return modules\n",
    "\n",
    "    def modules(self):\n",
    "        \"\"\"\n",
    "        Returns an iterator over all modules in the network.\n",
    "        \"\"\"\n",
    "        for name, module in self.named_modules():\n",
    "            yield module\n",
    "\n",
    "    def named_modules(self, memo=None, prefix=''):\n",
    "        \"\"\"\n",
    "        A named list of all modules.\n",
    "        \"\"\"\n",
    "        modules = {}\n",
    "        for k in self.__dict__:\n",
    "            v = self.__dict__[k]\n",
    "            if isinstance(v, Module):\n",
    "                modules[prefix + \".\" + k] = v\n",
    "                modules.update(**dict(v.named_modules(memo, prefix)))\n",
    "        return list(modules.items())\n",
    "\n",
    "    @RunOnlyOnce\n",
    "    def _register_params(self, module):\n",
    "        \"\"\"\n",
    "        Allows registration of the parameters with a native module.\n",
    "\n",
    "        This makes the parameters of a babilim modules available to the native modules.\n",
    "        When using a babilim modules in a native modules, use this function and pass the native module as a parameter.\n",
    "\n",
    "        This function works by adding all trainable_variables to the module you pass.\n",
    "        Warning: You need to build the babilim modules before calling this function. Building can be done by calling for example.\n",
    "\n",
    "        Here is a pytorch example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            import torch\n",
    "            from torch.nn import Module\n",
    "            from babilim.modules import Linear\n",
    "\n",
    "\n",
    "            class MyModule(Module):\n",
    "                def __init__(self):\n",
    "                    super().__init__()\n",
    "                    self.linear = Linear(10)\n",
    "\n",
    "                def forward(self, features):\n",
    "                    result = self.linear(features)\n",
    "                    self.linear.register_params(self)\n",
    "                    return result\n",
    "\n",
    "        :param module: The native module on which parameters of this modules should be registered.\n",
    "        \"\"\"\n",
    "        if babilim.is_backend(PYTORCH_BACKEND):\n",
    "            from torch.nn import Module\n",
    "            if isinstance(module, Module):\n",
    "                myname = \"_error_\"\n",
    "                for var in module.__dict__:\n",
    "                    if module.__dict__[var] == self:\n",
    "                        myname = var\n",
    "                    if isinstance(module.__dict__[var], list) and self in module.__dict__[var]:\n",
    "                        myname = \"{}/{}\".format(var, module.__dict__[var].index(self))\n",
    "\n",
    "                # Register self as pytorch module.\n",
    "                module._modules[myname] = self\n",
    "\n",
    "                for name, param in self.named_variables.items():\n",
    "                    if param.trainable:\n",
    "                        module.register_parameter(myname + name, param.native)\n",
    "                    else:\n",
    "                        module.register_buffer(myname + name, param.native)\n",
    "        else:\n",
    "            warn(\"Not implemented for tf2 but I think it is not required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

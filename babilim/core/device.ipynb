{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# babilim.core.device\n",
    "\n",
    "> Controll on what device code is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2019 Michael Fuerst\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "from typing import List\n",
    "import babilim\n",
    "from babilim import PYTORCH_BACKEND, TF_BACKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_device_stack = [\"gpu:0\"]\n",
    "\n",
    "def get_current_device() -> str:\n",
    "    \"\"\"\n",
    "    Get a string specifying the currently selected default device.\n",
    "    \n",
    "    When you manually assign a device, you should always use this device.\n",
    "    \"\"\"\n",
    "    return _device_stack[-1]\n",
    "\n",
    "def get_current_device_native_format() -> str:\n",
    "    \"\"\"\n",
    "    Get a string specifying the currently selected default device in the backend specific native format.\n",
    "    \n",
    "    When you manually assign a device, you should always use this device.\n",
    "    \"\"\"\n",
    "    name = _device_stack[-1]\n",
    "    if babilim.is_backend(TF_BACKEND):\n",
    "        return \"/\" + name\n",
    "    elif babilim.is_backend(PYTORCH_BACKEND):\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            return name.replace(\"gpu\", \"cuda\")\n",
    "        else:\n",
    "            return \"cpu\"\n",
    "    else:\n",
    "        raise RuntimeError(\"No implementation for this backend was found. (backend={})\".format(babilim.get_backend()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Device(object):\n",
    "    def __init__(self, name: str):\n",
    "        \"\"\"\n",
    "        Set the default device for babilim in a with statement.\n",
    "        \n",
    "        ```python\n",
    "        with Device(\"gpu:1\"):\n",
    "            # All tensors of MyModule are on gpu 1 automatically.\n",
    "            mymodule = MyModule()\n",
    "        ```\n",
    "        \n",
    "        When there is nested with-device statements, the innermost overwrites all others.\n",
    "        By default gpu:0 is used.\n",
    "        \n",
    "        Only works for tensors which are at some point wrapped by a babilim module (Lambda, Tensor, etc.).\n",
    "        \n",
    "        :param name: The name of the device. (\"cpu\", \"gpu:0\", \"gpu:1\", etc.)\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.native_device = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        _device_stack.append(self.name)\n",
    "        if babilim.is_backend(TF_BACKEND):\n",
    "            import tensorflow as tf\n",
    "            self.native_device = tf.device(get_current_device_native_format())\n",
    "            self.native_device.__enter__()\n",
    "        elif babilim.is_backend(PYTORCH_BACKEND):\n",
    "            pass\n",
    "        else:\n",
    "            raise RuntimeError(\"No implementation for this backend was found. (backend={})\".format(babilim.get_backend()))\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        _device_stack.pop()\n",
    "        if babilim.is_backend(TF_BACKEND):\n",
    "            self.native_device.__exit__()\n",
    "            self.native_device = None\n",
    "        elif babilim.is_backend(PYTORCH_BACKEND):\n",
    "            pass\n",
    "        else:\n",
    "            raise RuntimeError(\"No implementation for this backend was found. (backend={})\".format(babilim.get_backend()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

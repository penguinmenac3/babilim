{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# babilim.core.itensor\n",
    "\n",
    "> Tensor interface for babilim. Shared by tensorflow and pytorch.\n",
    "\n",
    "This code is under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2019 Michael Fuerst\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "from typing import Union, Any, Tuple, Optional, Sequence\n",
    "import numpy as np\n",
    "\n",
    "tensor_name_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ITensor(object):\n",
    "    def __init__(self, native):\n",
    "        \"\"\"\n",
    "        The babilim tensor api.\n",
    "        \n",
    "        **You must not instantiate this class**.\n",
    "        Instantiate babilim.core.Tensor and use babilim.core.ITensor for your type annotations.\n",
    "        Every babilim.core.Tensor implements the interface type babilim.core.ITensor.\n",
    "        \n",
    "        Beyond special operations every tensor implements common builtin functions like multiplication, division, addition, power etc.\n",
    "        \n",
    "        :param native: The native tensor that should be hidden behind this api.\n",
    "        \"\"\"\n",
    "        self.native = native\n",
    "    \n",
    "    def ref(self) -> object:\n",
    "        \"\"\"\n",
    "        Returns a hashable reference to this tensor.\n",
    "        \n",
    "        :return: A hashable tensor.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a tensor must implement this.\")\n",
    "\n",
    "    def copy(self) -> 'ITensor':\n",
    "        \"\"\"\n",
    "        Create a copy of the tensor.\n",
    "        \n",
    "        By creating a copy a new tensor which is not related to the old one (no gradients) will be created.\n",
    "        \n",
    "        :return: A copy of the tensor.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a tensor must implement this.\")\n",
    "\n",
    "    def cast(self, dtype) -> 'ITensor':\n",
    "        \"\"\"\n",
    "        Cast a tensor to a given dtype.\n",
    "        \n",
    "        :param dtype: The dtype as a string. Supported dtyles are \"float16\", \"float32\", \"float64\", \"bool\", \"uint8\", \"int8\", \"int16\", \"int32\" and \"int64\".\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a tensor must implement this.\")\n",
    "\n",
    "    def stop_gradients(self) -> 'ITensor':\n",
    "        \"\"\"\n",
    "        Returns a new tensor object sharing the value with the old one but without gradients connecting them.\n",
    "        \n",
    "        :return: A new tensor handle without gradients.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a tensor must implement this.\")\n",
    "\n",
    "    def assign(self, other: Union['ITensor', np.ndarray]) -> 'ITensor':\n",
    "        \"\"\"\n",
    "        Assign the values from another tensor to this tensor.\n",
    "        \n",
    "        :param other: The tensor providing the values for the assignment.\n",
    "        :return: Returns self for chaining of operations.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def reshape(self, shape) -> 'ITensor':\n",
    "        \"\"\"\n",
    "        Reshape a tensor into a given shape.\n",
    "\n",
    "        :param shape: (Tuple) The desired target shape.\n",
    "        :return: (ITensor) Returns a view on the tensor with the new shape.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a tensor must implement this.\")\n",
    "\n",
    "    def numpy(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts the tensor to a numpy array.\n",
    "        \n",
    "        :return: A numpy array with the contents of the tensor.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def mean(self, axis: Optional[int]=None) -> 'ITensor':\n",
    "        \"\"\"\n",
    "        Computes the mean operation on the tensor.\n",
    "        \n",
    "        :param axis: (Optional) An axis along which the mean should be computed. If none is given, all axis are reduced.\n",
    "        :return: A tensor containing the mean.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def argmax(self, axis: Optional[int]=None) -> 'ITensor':\n",
    "        \"\"\"\n",
    "        Computes the argmax operation on the tensor.\n",
    "        \n",
    "        :param axis: (Optional) An axis along which the argmax should be computed. If none is given, all axis are reduced.\n",
    "        :return: A tensor containing the argmax.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a tensor must implement this.\")\n",
    "\n",
    "    def sum(self, axis: Optional[int]=None) -> 'ITensor':\n",
    "        \"\"\"\n",
    "        Computes the sum operation on the tensor.\n",
    "        \n",
    "        :param axis: (Optional) An axis along which the sum should be computed. If none is given, all axis are reduced.\n",
    "        :return: A tensor containing the sum.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a tensor must implement this.\")\n",
    "\n",
    "    def is_nan(self) -> 'ITensor':\n",
    "        \"\"\"\n",
    "        Checks for each value in the tensor if it is not a number.\n",
    "        \n",
    "        :return: A tensor containing booleans if the value in the original tensors were nan.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a tensor must implement this.\")\n",
    "\n",
    "    def any(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if any vlaue of the tensor is true.\n",
    "        \n",
    "        :return: True if any value was true.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a tensor must implement this.\")\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple:\n",
    "        \"\"\"\n",
    "        Get the shape of the tensor.\n",
    "        \n",
    "        :return: A tuple representing the shape of the tensor.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    @property\n",
    "    def trainable(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a tensor is trainable.\n",
    "        \n",
    "        Essentially this is used to find out if a tensor should be updated during back propagation.\n",
    "        \n",
    "        :return: True if the tensor can be trained.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    # Binary Operators\n",
    "    def __add__(self, other: Union[float, 'ITensor']) -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __sub__(self, other: Union[float, 'ITensor']) -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __mul__(self, other: Union[float, 'ITensor']) -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __truediv__(self, other: Union[float, 'ITensor']) -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __mod__(self, other: Union[float, 'ITensor']) -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __pow__(self, other: Union[float, 'ITensor']) -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    # Comparison Operators\n",
    "    def __lt__(self, other: 'ITensor') -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __gt__(self, other: 'ITensor') -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __le__(self, other: 'ITensor') -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __ge__(self, other: 'ITensor') -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __eq__(self, other: 'ITensor') -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __ne__(self, other: 'ITensor') -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    # Unary Operators\n",
    "    def __neg__(self) -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __pos__(self) -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __invert__(self) -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    def __getitem__(self, item) -> 'ITensor':\n",
    "        raise NotImplementedError(\"Each implementation of a variable must implement this.\")\n",
    "\n",
    "    # Assignment Operators\n",
    "    def __isub__(self, other: Union[float, 'ITensor']) -> 'ITensor':\n",
    "        return self.assign(self - other)\n",
    "\n",
    "    def __iadd__(self, other: Union[float, 'ITensor']) -> 'ITensor':\n",
    "        return self.assign(self + other)\n",
    "\n",
    "    def __imul__(self, other: Union[float, 'ITensor']) -> 'ITensor':\n",
    "        return self.assign(self * other)\n",
    "\n",
    "    def __idiv__(self, other: Union[float, 'ITensor']) -> 'ITensor':\n",
    "        return self.assign(self / other)\n",
    "\n",
    "    def __imod__(self, other: Union[float, 'ITensor']) -> 'ITensor':\n",
    "        return self.assign(self % other)\n",
    "\n",
    "    def __ipow__(self, other: Union[float, 'ITensor']) -> 'ITensor':\n",
    "        return self.assign(self ** other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interoperability between Pytorch/Tensorflow and Babilim\n",
    "\n",
    "Sometimes it is nescesarry to implement stuff in native pytorch or native tensorflow. Here the tensor wrapper can help.\n",
    "\n",
    "**WARNING: Instead of directly using the TensorWrapper, you should prefer using the babilim.module.Lambda!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ITensorWrapper(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This interface implements functions required to wrap variables from native pytorch/tensorflow code for babilim.\n",
    "        \"\"\"\n",
    "        raise RuntimeError(\"This interface must not be instantiated!\")\n",
    "    \n",
    "    def wrap(self, obj: Any) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Wrap simple datatypes into a tensor.\n",
    "        \n",
    "        Supported datatypes are Tuple, Sequence, Dict, native tensors and numpy arrays.\n",
    "        \n",
    "        For Tuple, Dict, Sequence it wraps every element of the object recursively.\n",
    "        For native tensors and numpy arrays it wraps them as an ITensor.\n",
    "        \n",
    "        :param obj: The object that should be wrapped.\n",
    "        :return: The wrapped object or none if object cannot be wrapped.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def unwrap(self, obj: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Unwrap an object.\n",
    "        \n",
    "        This function is the inverse to wrap (if the object was wrapped).\n",
    "        \n",
    "        :return: The unwrapped object again.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def is_variable(self, obj: Any) -> bool:\n",
    "        \"\"\"\n",
    "        Check if an object is a variable in the framework.\n",
    "        \n",
    "        :param obj: The object that should be tested.\n",
    "        :return: True if the object is a tensorflow/pytorch variable.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def wrap_variable(self, obj: Any) -> 'ITensor':\n",
    "        \"\"\"\n",
    "        Wrap a variable as an ITensor.\n",
    "        \n",
    "        :param obj: The object that should be wrapped.\n",
    "        :return: An ITensor object containing the variable.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def vars_from_object(self, obj: Any, namespace: str) -> Sequence[Tuple[str, 'ITensor']]:\n",
    "        \"\"\"\n",
    "        Get all variables in an native module.\n",
    "        \n",
    "        This function retrieves all variables from a native module and converts them in a list of mappings from name to ITensor.\n",
    "        \n",
    "        :param obj: The native module from which to extract the variables.\n",
    "        :param namespace: The namespace that should be used for the variables found.\n",
    "        :return: A list containing tuples mapping from a name (str) to the ITensor.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# babilim.core.module\n",
    "\n",
    "> An object which can have a state that is trainable or checkpointable. The core unit of babilim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Sequence, Any, Sequence, Callable, Dict, Iterable\n",
    "from collections import defaultdict, OrderedDict\n",
    "import inspect\n",
    "import numpy as np\n",
    "\n",
    "import babilim\n",
    "from babilim import PYTORCH_BACKEND, TF_BACKEND\n",
    "from babilim.core.annotations import RunOnlyOnce\n",
    "from babilim.core.device import get_current_device_native_format, Device\n",
    "from babilim.core.checkpoint import load_state, save_state\n",
    "from babilim.core.itensor import ITensor\n",
    "from babilim.core.tensor import Tensor, TensorWrapper\n",
    "from babilim.core.logging import info, warn, DEBUG_VERBOSITY\n",
    "\n",
    "\n",
    "@RunOnlyOnce\n",
    "def _warn_once(msg):\n",
    "    warn(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_statefull_object_name_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        A module is an object with variables that can be trainable and checkpointable.\n",
    "\n",
    "        Furthermore every module is callable.\n",
    "        A module can be used with native or babilim tensors when the callable api is used.\n",
    "        It automatically wraps native tensors and calls the `call` function.\n",
    "        \n",
    "        Attributes:\n",
    "        * `self.initialized_module`: A boolen storing if the module is already initialized. When not initialized loading state will fail.\n",
    "        * `self.device`: Specifies the device on which this module is.\n",
    "        \"\"\"\n",
    "        self._wrapper = TensorWrapper()\n",
    "        self._training = True\n",
    "        self.initialized_module = False\n",
    "        self.device = get_current_device_native_format()\n",
    "\n",
    "    def initialize(self, dataset):\n",
    "        \"\"\"\n",
    "        Initializes your module by running a sample of your dataset through it.\n",
    "\n",
    "        :param dataset: The dataset you want to use for initialization. (Must be of type babilim.data.Dataset)\n",
    "        \"\"\"\n",
    "        if not self.initialized_module:\n",
    "            if babilim.core.logging.DEBUG_VERBOSITY:\n",
    "                info(\"Build Model\")\n",
    "            self.initialized_module = True\n",
    "            dataloader = dataset.to_dataloader()\n",
    "            features, _ = next(iter(dataloader))\n",
    "            self(**features._asdict())\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Makes a module callable. Automatically wraps tensorflow or pytorch tensors to ITensors from babilim.\n",
    "\n",
    "        ```python\n",
    "        module = MyModule()\n",
    "        module(*args, **kwargs)\n",
    "        ```\n",
    "\n",
    "        Warning: This function should not be overwritten. Instead overwrite `call` with no underscores.\n",
    "\n",
    "        :param *args: All indexed parameters of your call function derivate.\n",
    "        :param **kwargs: All named parameters of your call function derivate.\n",
    "        \"\"\"\n",
    "        with Device(self.device):\n",
    "            # ensure that call gets called with ITensor objects but the caller can use native tensors.\n",
    "            args_wrapped = self._wrapper.wrap(args)\n",
    "            kwargs_wrapped = self._wrapper.wrap(kwargs)\n",
    "            wrapped = False\n",
    "            if args_wrapped is not None and len(args) > 0:\n",
    "                wrapped = True\n",
    "                args = args_wrapped\n",
    "            if kwargs_wrapped is not None and len(kwargs) > 0:\n",
    "                wrapped = True\n",
    "                kwargs = kwargs_wrapped\n",
    "\n",
    "            self.build(*args, **kwargs)\n",
    "            result = self.call(*args, **kwargs)\n",
    "            parent_dict = inspect.stack()[1][0].f_locals\n",
    "            if \"self\" in parent_dict:\n",
    "                parent = parent_dict[\"self\"]\n",
    "                self._register_params(parent)\n",
    "            if wrapped:\n",
    "                return self._wrapper.unwrap(result)\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "    def build(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        This function will build your model and must be annotated with the RunOnlyOnce-Annotation.\n",
    "\n",
    "        Allocating weight tensors should be done here.\n",
    "        You can make use of the knowledge of your inputs to compute shapes for your weight tensors.\n",
    "        This will make coding dimensions a lot easier.\n",
    "\n",
    "        ```python\n",
    "        @RunOnlyOnce\n",
    "        def build(self, image: ITensor) -> None:\n",
    "        ```\n",
    "\n",
    "        :param *args: You must specify the exact same parameters as for your call.\n",
    "        :param **kwargs: You must specify the exact same parameters as for your call.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def call(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Makes a module callable and contains the forward pass of your model.\n",
    "        This should be pure computation and not allocate any weights.\n",
    "        Allocating weights should be done in the `build` function.\n",
    "\n",
    "        This function gets called by `__call__` and must be overwritten by any derived class.\n",
    "\n",
    "        ```python\n",
    "        def call(self, image: ITensor) -> NetworkOutput:\n",
    "        ```\n",
    "\n",
    "        :param *args: You can specify any parameters you want.\n",
    "        :param **kwargs: You can specify any named parameters you want.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Every modules must implement this method.\")\n",
    "\n",
    "    def predict(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Pass in single training examples as numpy arrays.\n",
    "        And predict the value without gradients.\n",
    "        Should be used for testing and evaluation.\n",
    "\n",
    "        If your network has eval modes you need to set them manually.\n",
    "\n",
    "        The array must not have batch dimension.\n",
    "\n",
    "        :param kwargs: The parameters to feed the network as a single example.\n",
    "        :return: The output for a single example.\n",
    "        \"\"\"\n",
    "        with Device(self.device):\n",
    "            kwargs = {k: np.array([kwargs[k]]) for k in kwargs.keys() if isinstance(kwargs[k], np.ndarray)}\n",
    "            kwargs = {k: Tensor(data=kwargs[k], trainable=False) for k in kwargs.keys()}\n",
    "\n",
    "            preds = self.__call__(**kwargs)\n",
    "            tmp = preds._asdict()\n",
    "            tmp = {k: tmp[k].numpy()[0] for k in tmp.keys()}\n",
    "            preds = type(preds)(**tmp)\n",
    "            return preds\n",
    "\n",
    "    @property\n",
    "    def submodules(self):\n",
    "        \"\"\"\n",
    "        A property to get all submodules.\n",
    "        \n",
    "        A submodule is a module stored in an attribute of a module.\n",
    "        \n",
    "        ```python\n",
    "        module.submodules\n",
    "        ```\n",
    "        \"\"\"\n",
    "        modules = []\n",
    "        for k in self.__dict__:\n",
    "            v = self.__dict__[k]\n",
    "            if isinstance(v, Module):\n",
    "                modules.append(v)\n",
    "                modules.append(v.submodules)\n",
    "        return modules\n",
    "\n",
    "    def modules(self):\n",
    "        \"\"\"\n",
    "        Returns an iterator over all submodules in the module.\n",
    "        \n",
    "        A submodule is a module stored in an attribute of a module.\n",
    "        \"\"\"\n",
    "        for name, module in self.named_modules():\n",
    "            yield module\n",
    "\n",
    "    def named_modules(self, memo=None, prefix=''):\n",
    "        \"\"\"\n",
    "        A named list of all submodules.\n",
    "        \n",
    "        A submodule is a module stored in an attribute of a module.\n",
    "        \"\"\"\n",
    "        modules = {}\n",
    "        for k in self.__dict__:\n",
    "            v = self.__dict__[k]\n",
    "            if isinstance(v, Module):\n",
    "                modules[prefix + \".\" + k] = v\n",
    "                modules.update(**dict(v.named_modules(memo, prefix)))\n",
    "        return list(modules.items())\n",
    "\n",
    "    @RunOnlyOnce\n",
    "    def _register_params(self, module):\n",
    "        \"\"\"\n",
    "        Allows registration of the parameters with a native module.\n",
    "\n",
    "        This makes the parameters of a babilim modules available to the native modules.\n",
    "        When using a babilim modules in a native modules, use this function and pass the native module as a parameter.\n",
    "\n",
    "        This function works by adding all trainable_variables to the module you pass.\n",
    "        Warning: You need to build the babilim modules before calling this function. Building can be done by calling for example.\n",
    "\n",
    "        Here is a pytorch example:\n",
    "\n",
    "        ```python\n",
    "        import torch\n",
    "        from torch.nn import Module\n",
    "        from babilim.modules import Linear\n",
    "\n",
    "        class MyModule(Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.linear = Linear(10)\n",
    "\n",
    "            def forward(self, features):\n",
    "                result = self.linear(features)\n",
    "                self.linear.register_params(self)\n",
    "                return result\n",
    "        ```\n",
    "\n",
    "        :param module: The native module on which parameters of this modules should be registered.\n",
    "        \"\"\"\n",
    "        if babilim.is_backend(PYTORCH_BACKEND):\n",
    "            from torch.nn import Module\n",
    "            if isinstance(module, Module):\n",
    "                myname = \"_error_\"\n",
    "                for var in module.__dict__:\n",
    "                    if module.__dict__[var] == self:\n",
    "                        myname = var\n",
    "                    if isinstance(module.__dict__[var], list) and self in module.__dict__[var]:\n",
    "                        myname = \"{}/{}\".format(var, module.__dict__[var].index(self))\n",
    "\n",
    "                # Register self as pytorch module.\n",
    "                module._modules[myname] = self\n",
    "\n",
    "                for name, param in self.named_variables.items():\n",
    "                    if param.trainable:\n",
    "                        module.register_parameter(myname + name, param.native)\n",
    "                    else:\n",
    "                        module.register_buffer(myname + name, param.native)\n",
    "        else:\n",
    "            if babilim.core.logging.DEBUG_VERBOSITY:\n",
    "                _warn_once(\"babilim.model.module.Module:_register_params Not implemented for tf2 but I think it is not required.\")\n",
    "        \n",
    "    @property\n",
    "    def training(self) -> bool:\n",
    "        \"\"\"\n",
    "        Property if the object is in training mode.\n",
    "        \n",
    "        ```python\n",
    "        module.training\n",
    "        ```\n",
    "        \n",
    "        :return: True if the object is in training mode.\n",
    "        \"\"\"\n",
    "        return self._training\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        \"\"\"\n",
    "        Property with all variables of the object.\n",
    "        \n",
    "        ```python\n",
    "        module.variables\n",
    "        ```\n",
    "        \n",
    "        :return: A list of the variables in this object.\n",
    "        \"\"\"\n",
    "        return list(self.named_variables.values())\n",
    "\n",
    "    @property\n",
    "    def named_variables(self):\n",
    "        \"\"\"\n",
    "        Property with all variables of the object.\n",
    "        \n",
    "        ```python\n",
    "        module.named_variables\n",
    "        ```\n",
    "        \n",
    "        :return: A dictionary of the variables in this object.\n",
    "        \"\"\"\n",
    "        return dict(self.__variables_with_namespace())\n",
    "\n",
    "    def __variables_with_namespace(self, namespace=\"\"):\n",
    "        all_vars = []\n",
    "        extra_vars = []\n",
    "        for member_name in self.__dict__:\n",
    "            v = self.__dict__[member_name]\n",
    "            if isinstance(v, str):\n",
    "                pass\n",
    "            elif isinstance(v, Dict):\n",
    "                for i, (k, x) in enumerate(v.items()):\n",
    "                    if not isinstance(k, str):\n",
    "                        k = \"{}\".format(i)\n",
    "                    name = namespace + \"/\" + member_name + \"/\" + k\n",
    "                    if isinstance(x, Module):\n",
    "                        all_vars.extend(x.__variables_with_namespace(name))\n",
    "                    if isinstance(x, ITensor):\n",
    "                        all_vars.append((name, x))\n",
    "                    if self._wrapper.is_variable(x):\n",
    "                        extra_vars.append((name, self._wrapper.wrap_variable(x)))\n",
    "                    if isinstance(x, object):\n",
    "                        extra_vars.extend(self._wrapper.vars_from_object(v, name))\n",
    "            elif isinstance(v, Iterable):\n",
    "                for i, x in enumerate(v):\n",
    "                    name = namespace + \"/\" + member_name + \"/{}\".format(i)\n",
    "                    if isinstance(x, Module):\n",
    "                        all_vars.extend(x.__variables_with_namespace(name))\n",
    "                    if isinstance(x, ITensor):\n",
    "                        all_vars.append((name, x))\n",
    "                    if self._wrapper.is_variable(x):\n",
    "                        extra_vars.append((name, self._wrapper.wrap_variable(x)))\n",
    "                    if isinstance(x, object):\n",
    "                        extra_vars.extend(self._wrapper.vars_from_object(v, name))\n",
    "            elif isinstance(v, Module):\n",
    "                name = namespace + \"/\" + member_name\n",
    "                all_vars.extend(v.__variables_with_namespace(name))\n",
    "            elif isinstance(v, ITensor):\n",
    "                name = namespace + \"/\" + member_name\n",
    "                all_vars.append((name, v))\n",
    "            # Native only counts if there is no babilim stuff.\n",
    "            elif self._wrapper.is_variable(v):\n",
    "                name = namespace + \"/\" + member_name\n",
    "                extra_vars.append((name, self._wrapper.wrap_variable(v)))\n",
    "            elif isinstance(v, object):\n",
    "                name = namespace + \"/\" + member_name\n",
    "                extra_vars.extend(self._wrapper.vars_from_object(v, name))\n",
    "                for x in getattr(v, '__dict__', {}):\n",
    "                    name = namespace + \"/\" + member_name + \"/\" + x\n",
    "                    if isinstance(v.__dict__[x], Module):\n",
    "                        all_vars.extend(v.__dict__[x].__variables_with_namespace(name))\n",
    "                    if isinstance(v.__dict__[x], ITensor):\n",
    "                        all_vars.append((name, v.__dict__[x]))\n",
    "                    if self._wrapper.is_variable(v.__dict__[x]):\n",
    "                        extra_vars.append((name, self._wrapper.wrap_variable(v.__dict__[x])))\n",
    "        if len(all_vars) == 0:\n",
    "            all_vars.extend(extra_vars)\n",
    "        return all_vars\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        \"\"\"\n",
    "        Property with trainable variables of the object.\n",
    "        \n",
    "        ```python\n",
    "        module.trainable_variables\n",
    "        ```\n",
    "        \n",
    "        :return: A list of the trainable variables in this object.\n",
    "        \"\"\"\n",
    "        all_vars = self.variables\n",
    "        train_vars = []\n",
    "        for v in all_vars:\n",
    "            if v.trainable:\n",
    "                train_vars.append(v)\n",
    "        return train_vars\n",
    "\n",
    "    @property\n",
    "    def named_trainable_variables(self):\n",
    "        \"\"\"\n",
    "        Property with trainable variables of the object.\n",
    "        \n",
    "        ```python\n",
    "        module.named_trainable_variables\n",
    "        ```\n",
    "        \n",
    "        :return: A dictionary of the trainable variables in this object.\n",
    "        \"\"\"\n",
    "        all_vars = self.named_variables\n",
    "        train_vars = []\n",
    "        for k, v in all_vars.items():\n",
    "            if v.trainable:\n",
    "                train_vars.append((k, v))\n",
    "        return dict(train_vars)\n",
    "\n",
    "    @property\n",
    "    def untrainable_variables(self):\n",
    "        \"\"\"\n",
    "        Property with not trainable variables of the object.\n",
    "        \n",
    "        ```python\n",
    "        module.untrainable_variables\n",
    "        ```\n",
    "        \n",
    "        :return: A list of not trainable variables in this object.\n",
    "        \"\"\"\n",
    "        all_vars = self.variables\n",
    "        train_vars = []\n",
    "        for v in all_vars:\n",
    "            if not v.trainable:\n",
    "                train_vars.append(v)\n",
    "        return train_vars\n",
    "\n",
    "    @property\n",
    "    def named_untrainable_variables(self):\n",
    "        \"\"\"\n",
    "        Property with not trainable variables of the object.\n",
    "        \n",
    "        ```python\n",
    "        module.named_untrainable_variables\n",
    "        ```\n",
    "        \n",
    "        :return: A dictionary of not trainable variables in this object.\n",
    "        \"\"\"\n",
    "        all_vars = self.named_variables\n",
    "        train_vars = []\n",
    "        for k, v in all_vars.items():\n",
    "            if not v.trainable:\n",
    "                train_vars.append((k, v))\n",
    "        return dict(train_vars)\n",
    "\n",
    "    @property\n",
    "    def trainable_variables_native(self):\n",
    "        \"\"\"\n",
    "        Property with not trainable variables of the object in native format.\n",
    "        \n",
    "        ```python\n",
    "        module.trainable_variables_native\n",
    "        ```\n",
    "        \n",
    "        :return: A list of trainable variables in this object in native format.\n",
    "        \"\"\"\n",
    "        all_vars = self.trainable_variables\n",
    "        train_vars = []\n",
    "        for v in all_vars:\n",
    "            train_vars.append(v.native)\n",
    "        return train_vars\n",
    "\n",
    "    @property\n",
    "    def _parameters(self) -> OrderedDict:\n",
    "        params = OrderedDict()\n",
    "        params.update(self.named_trainable_variables)\n",
    "        return params\n",
    "\n",
    "    @property\n",
    "    def _buffers(self) -> OrderedDict:\n",
    "        params = OrderedDict()\n",
    "        params.update(self.named_untrainable_variables)\n",
    "        return params\n",
    "\n",
    "    def state_dict(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the state of the object as a state dict (usable for checkpoints).\n",
    "        \n",
    "        :return: A dictionary containing the state of the object.\n",
    "        \"\"\"\n",
    "        state = {}\n",
    "        for name, var in self.named_variables.items():\n",
    "            if babilim.is_backend(babilim.TF_BACKEND):\n",
    "                state[name] = var.numpy()\n",
    "            else:\n",
    "                state[name] = var.numpy().T\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, state_dict: Dict) -> None:\n",
    "        \"\"\"\n",
    "        Load the state of the object from a state dict.\n",
    "        \n",
    "        Handy when loading checkpoints.\n",
    "        \n",
    "        :param state_dict: A dictionary containing the state of the object.\n",
    "        \"\"\"\n",
    "        for name, var in self.named_variables.items():\n",
    "            if name in state_dict:\n",
    "                if babilim.is_backend(babilim.TF_BACKEND):\n",
    "                    var.assign(state_dict[name])\n",
    "                else:\n",
    "                    var.assign(state_dict[name].T)\n",
    "                if DEBUG_VERBOSITY:\n",
    "                    info(\"  Loaded: {}\".format(name))\n",
    "            else:\n",
    "                warn(\"  Variable {} not in checkpoint.\".format(name))\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Set the object into eval mode.\n",
    "        \n",
    "        ```python\n",
    "        self.train(False)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        self.train(False)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"\n",
    "        Set the objects training mode.\n",
    "        \n",
    "        :param mode: (Optional) If the training mode is enabled or disabled. (default: True)\n",
    "        \"\"\"\n",
    "        self._training = mode\n",
    "        for member_name in self.__dict__:\n",
    "            obj = self.__dict__[member_name]\n",
    "            if isinstance(obj, Sequence):\n",
    "                for x in obj:\n",
    "                    train_fn = getattr(x, \"train\", None)\n",
    "                    if callable(train_fn):\n",
    "                        train_fn(mode)\n",
    "            else:\n",
    "                train_fn = getattr(obj, \"train\", None)\n",
    "                if callable(train_fn):\n",
    "                    train_fn(mode)\n",
    "\n",
    "    def load(self, checkpoint_file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load the state of the object from a checkpoint.\n",
    "        \n",
    "        :param checkpoint_file_path: The path to the checkpoint storing the state dict.\n",
    "        \"\"\"\n",
    "        checkpoint = load_state(checkpoint_file_path)\n",
    "        if \"model\" in checkpoint:\n",
    "            self.load_state_dict(checkpoint[\"model\"])\n",
    "        else:\n",
    "            babilim.error(\"Could not find state in checkpoint.\")\n",
    "\n",
    "    def save(self, checkpoint_file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the state of the object to a checkpoint.\n",
    "        \n",
    "        :param checkpoint_file_path: The path to the checkpoint storing the state dict.\n",
    "        \"\"\"\n",
    "        save_state({\"model\": self.state_dict()}, checkpoint_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

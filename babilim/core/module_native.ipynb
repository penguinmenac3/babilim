{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# babilim.core.module_native\n",
    "\n",
    "> A module that is implemented by native function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Any\n",
    "\n",
    "import babilim\n",
    "from babilim import PYTORCH_BACKEND, TF_BACKEND, is_backend, get_backend\n",
    "from babilim.core.annotations import RunOnlyOnce\n",
    "from babilim.core.module import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ModuleNative(Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        A module with a native implementation.\n",
    "        \n",
    "        This module is like a normal module, except that call and build call a \"call_pytorch\", \"call_tf\", \"build_pytorch\" and \"build_tf\" depending on what backend is set.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "    @RunOnlyOnce\n",
    "    def build(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Build the model, this function automatically calls the native build with the tensors unwrapped.\n",
    "\n",
    "        :param *args: You must specify the exact same parameters as for your call.\n",
    "        :param **kwargs: You must specify the exact same parameters as for your call.\n",
    "        \"\"\"\n",
    "        args = self._wrapper.unwrap(args)\n",
    "        kwargs = self._wrapper.unwrap(kwargs)\n",
    "        if is_backend(PYTORCH_BACKEND):\n",
    "            build_pytorch(*args, **kwargs)\n",
    "        elif is_backend(TF_BACKEND):\n",
    "            build_tf(*args, **kwargs)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown Backend: {}\".format(get_backend()))\n",
    "            \n",
    "    def build_pytorch(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        A native build function in pytorch.\n",
    "        \n",
    "        Even though babilim never calls this function directly multiple times, it is recommended to add the RunOnlyOnce guard in case a user calls it multiple times.\n",
    "        \n",
    "        :param *args: You must specify the exact same parameters as for your call.\n",
    "        :param **kwargs: You must specify the exact same parameters as for your call.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def build_tf(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        A native build function in tensorflow.\n",
    "        \n",
    "        Even though babilim never calls this function directly multiple times, it is recommended to add the RunOnlyOnce guard in case a user calls it multiple times.\n",
    "        \n",
    "        :param *args: You must specify the exact same parameters as for your call.\n",
    "        :param **kwargs: You must specify the exact same parameters as for your call.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def call(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Makes a module callable and contains the forward pass of your model.\n",
    "        This should be pure computation and not allocate any weights.\n",
    "        Allocating weights should be done in the `build` function.\n",
    "\n",
    "        This function gets called by `__call__` and must be overwritten by any derived class.\n",
    "\n",
    "        ```python\n",
    "        def call(self, image: ITensor) -> NetworkOutput:\n",
    "        ```\n",
    "\n",
    "        :param *args: You can specify any parameters you want.\n",
    "        :param **kwargs: You can specify any named parameters you want.\n",
    "        \"\"\"\n",
    "        args = self._wrapper.unwrap(args)\n",
    "        kwargs = self._wrapper.unwrap(kwargs)\n",
    "        if is_backend(PYTORCH_BACKEND):\n",
    "            results = call_pytorch(*args, **kwargs)\n",
    "        elif is_backend(TF_BACKEND):\n",
    "            results = call_tf(*args, **kwargs)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown Backend: {}\".format(get_backend()))\n",
    "        \n",
    "        results = self._wrapper.wrap(results)\n",
    "        return results\n",
    "    \n",
    "    def call_pytorch(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        A native call function in pytorch (like the forward).\n",
    "        \n",
    "        :param *args: You must specify the exact same parameters as for your call.\n",
    "        :param **kwargs: You must specify the exact same parameters as for your call.\n",
    "        \"\"\"\n",
    "        raise NotImplemented()\n",
    "    \n",
    "    def build_tf(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        A native call function in tensorflow.\n",
    "        \n",
    "        :param *args: You must specify the exact same parameters as for your call.\n",
    "        :param **kwargs: You must specify the exact same parameters as for your call.\n",
    "        \"\"\"\n",
    "        raise NotImplemented()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

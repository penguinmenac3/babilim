{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# babilim.data.dataset\n",
    "\n",
    "> A base class for implementing datasets with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2019 Michael Fuerst\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Any, Sequence, Tuple\n",
    "import traceback\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import babilim\n",
    "from babilim.core.logging import info, status\n",
    "from babilim.core.config import Config\n",
    "from babilim.data.dataloader import Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset(Sequence):\n",
    "    def __init__(self, config: Config, cache_dir: str = None):\n",
    "        \"\"\"\n",
    "        An abstract class representing a Dataset.\n",
    "\n",
    "        All other datasets must subclass it. All subclasses must override\n",
    "        `__len__`, that provides the size of the dataset, and `getitem`,\n",
    "        supporting integer indexing in range from 0 to len(self) exclusive and `_get_version`.\n",
    "\n",
    "        Extending on the pytorch dataset this dataset also needs to implement a `version` function.\n",
    "        The version function returns a number (can be a hash) which changes, whenever the dataset changes.\n",
    "        This enables subsequent callers to buffer this dataset and update their buffers when the version changes.\n",
    "        \n",
    "        A dataset loads the data from the disk as general as possible and then transformers adapt it to the needs of the neural network.\n",
    "        There are two types of transformers (which are called in the order listed here):\n",
    "        * `self.transformers = []`: These transformers are applied once on the dataset (before caching is done).\n",
    "        * `self.realtime_transformers = []`: These transformers are applied every time a sample is retrieved. (e.g. random data augmentations)\n",
    "        \n",
    "        :param config: The configuration used for your problem. (The problem parameters and train_batch_size are relevant for data loading.)\n",
    "        :param cache_dir: The directory where the dataset can cache itself. Caching allows faster loading, when complex transformations are required.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.transformers = []\n",
    "        self.realtime_transformers = []\n",
    "        self._caching = False\n",
    "        self._cache_dir = cache_dir\n",
    "        self._cache_indices = {}\n",
    "        self._cached_len = -1\n",
    "        if self._cache_dir is not None:\n",
    "            self.init_caching(cache_dir)\n",
    "            self._cached_len = len(self._cache_indices)\n",
    "\n",
    "    def init_caching(self, cache_dir):\n",
    "        \"\"\"\n",
    "        Initialize caching for quicker access once the data was cached once.\n",
    "        \n",
    "        The caching caches the calls to the getitem including application of regular transformers.\n",
    "        When calling this function the cache gets read if it exists or otherwise the folder is created and on first calling the getitem the item is stored.\n",
    "        \n",
    "        :param cache_dir: Directory where the cache should be stored.\n",
    "        \"\"\"\n",
    "        info(\"Init caching: {}\".format(cache_dir))\n",
    "        self._caching = True\n",
    "        self._cache_dir = cache_dir\n",
    "        # If it does not exist create the cache dir.\n",
    "        if not os.path.exists(self._cache_dir):\n",
    "            os.makedirs(self._cache_dir)\n",
    "\n",
    "        # Read all files in the folder into a dict that maps indices to filenames (for quicker access)\n",
    "        cache_files = os.listdir(self._cache_dir)\n",
    "        for cf in cache_files:\n",
    "            if cf.endswith(\".pk\"):\n",
    "                self._cache_indices[int(cf.replace(\".pk\", \"\"))] = os.path.join(self._cache_dir, cf)\n",
    "\n",
    "    def _cache(self, index: int, value) -> None:\n",
    "        fp = os.path.join(self._cache_dir, \"{:09d}.pk\".format(index))\n",
    "        with open(fp, \"wb\") as f:\n",
    "            pickle.dump(value, f)\n",
    "        self._cache_indices[index] = fp\n",
    "\n",
    "    def getitem(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Gets called by `__getitem__`.\n",
    "        \n",
    "        This function must be overwritten by subclasses.\n",
    "        It loads a training sample given an index in the dataset.\n",
    "        \n",
    "        Never overwrite `__getitem__` directly, as it handles the caching and application of transformers.\n",
    "        \n",
    "        :param index: The index between 0 and len(self), identifying the sample that should be loaded.\n",
    "        :return: A tuple of features and values for the neural network. Features must be of type InputType (namedtuple) and labels of type InputType(namedtuple).\n",
    "        \"\"\"\n",
    "        if self._caching:\n",
    "            raise KeyError(\"The cache for index '{}' is missing.\".format(index))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        # Check if len is exceeded.\n",
    "        if index >= len(self):\n",
    "            raise IndexError()\n",
    "\n",
    "        if self._caching and index in self._cache_indices:\n",
    "            with open(self._cache_indices[index], \"rb\") as f:\n",
    "                sample = pickle.load(f)\n",
    "        else:\n",
    "            # Print index errors, they probably were an error and not intentional.\n",
    "            try:\n",
    "                sample = self.getitem(index)\n",
    "            except IndexError as e:\n",
    "                traceback.print_exc(file=sys.stderr)\n",
    "                raise e\n",
    "\n",
    "            # Apply transforms if they are available.\n",
    "            for transform in self.transformers:\n",
    "                sample = transform(*sample)\n",
    "\n",
    "            if self._caching:\n",
    "                return self._cache(index, sample)\n",
    "\n",
    "        # Apply real time transformers after caching. Realtime is not cached\n",
    "        for transform in self.realtime_transformers:\n",
    "            sample = transform(*sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self._cached_len >= 0:\n",
    "            return self._cached_len\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def version(self) -> str:\n",
    "        \"\"\"\n",
    "        Property that returns the version of the dataset.\n",
    "        \n",
    "        **You must not overwrite this, instead overwrite `_get_version(self) -> str` used by this property.**\n",
    "\n",
    "        :return: The version number of the dataset.\n",
    "        \"\"\"\n",
    "        version = \"{}\".format(self._get_version())\n",
    "        for transform in self.transformers:\n",
    "            version = \"{}_{}\".format(version, transform.version)\n",
    "        for transform in self.realtime_transformers:\n",
    "            version = \"{}_{}\".format(version, transform.version)\n",
    "        return version\n",
    "\n",
    "    def _get_version(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the version string of the dataset.\n",
    "        \n",
    "        **Must be overwritten by every subclass.**\n",
    "\n",
    "        :return: The version number of the dataset.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def to_dataloader(self) -> Dataloader:\n",
    "        \"\"\"\n",
    "        Converts the dataset into a babilim.data.Dataloader.\n",
    "        \n",
    "        :return: Returns a babilim dataloader object usable with the trainers.\n",
    "        \"\"\"\n",
    "        data = None\n",
    "        if babilim.is_backend(babilim.PYTORCH_BACKEND):\n",
    "            data = self.to_pytorch()\n",
    "        elif babilim.is_backend(babilim.TF_BACKEND):\n",
    "            data = self.to_keras()\n",
    "        else:\n",
    "            raise NotImplementedError(\"Other backends than pytorch and tf2 are not implemented.\")\n",
    "        return Dataloader(data, self)\n",
    "\n",
    "    def to_keras(self):\n",
    "        \"\"\"\n",
    "        Converts the dataset into a batched keras dataset.\n",
    "        \n",
    "        You can use this if you want to use a babilim dataset without babilim natively in keras.\n",
    "        \n",
    "        :return: The type will be tf.keras.Sequence.\n",
    "        \"\"\"\n",
    "        from babilim.data.keras import BatchedKerasDataset\n",
    "        return BatchedKerasDataset(self, self.config)\n",
    "\n",
    "    def to_pytorch(self):\n",
    "        \"\"\"\n",
    "        Converts the dataset into a batched pytorch dataset.\n",
    "        \n",
    "        You can use this if you want to use a babilim dataset without babilim natively in pytorch.\n",
    "        \n",
    "        :return: The type will be torch.utils.data.DataLoader.\n",
    "        \"\"\"\n",
    "        from babilim.data.pytorch import BatchedPytorchDataset\n",
    "        return BatchedPytorchDataset(self, self.config, self.config.problem_shuffle, self.config.problem_num_threads)\n",
    "\n",
    "\n",
    "    def to_tfrecord(self):\n",
    "        \"\"\"\n",
    "        Creates a tfrecord dataset from the dataset.\n",
    "\n",
    "        **Currently not implemented. Use Dataset.from_disk(...).to_keras() instead.**\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"This is not implemented yet. Use Dataset.from_disk(...).to_keras() instead.\")\n",
    "\n",
    "    def to_disk(self, cache_path: str, verbose: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Write a dataset as a cache to the disk.\n",
    " \n",
    "        :param cache_path: The path where the cache should be written.\n",
    "        :param verbose: If info on progress should be printed, defaults to True.\n",
    "        \"\"\"\n",
    "        self.init_caching(cache_path)\n",
    "        if verbose:\n",
    "            info(\"Caching dataset to {}\".format(cache_path))\n",
    "        N = len(self)\n",
    "        for i, _ in enumerate(self):\n",
    "            if verbose:\n",
    "                status(\"{}/{}\".format(i, N), end=\"\")\n",
    "        \n",
    "        if verbose:\n",
    "            info(\"\")\n",
    "            info(\"Caching done.\")\n",
    " \n",
    "    @staticmethod\n",
    "    def from_disk(config: Config, cache_path: str) -> 'Dataset':\n",
    "        \"\"\"\n",
    "        Create a dataset from a cache on disk.\n",
    "\n",
    "        :param config: The configuration for the dataset.\n",
    "        :param cache_path: The path to the cache.\n",
    "        :param version: The version of the dataset that should be loaded.\n",
    "        :return: A Dataset object that represents the data that has been passed to \"to_disk\" when creating the cache.\n",
    "        \"\"\"\n",
    "        return Dataset(config, cache_dir=cache_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

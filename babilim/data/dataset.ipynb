{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# babilim.data.dataset\n",
    "\n",
    "> A base class for implementing datasets with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2019 Michael Fuerst\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Any, Sequence, Tuple, NamedTuple\n",
    "import traceback\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import babilim\n",
    "from babilim.core.logging import info, status\n",
    "from babilim.core.config import Config\n",
    "from babilim.data.dataloader import Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset(Sequence):\n",
    "    def __init__(self, config: Config, dataset_input_type=NamedTuple, dataset_output_type=NamedTuple, cache_dir: str = None):\n",
    "        \"\"\"\n",
    "        An abstract class representing a Dataset.\n",
    "\n",
    "        All other datasets must subclass it.\n",
    "        Must overwrite `_get_version` and implement getters for the fields supported in the dataset_input_type and\n",
    "        dataset_output_type. Getters must be following this name schema:\n",
    "        \"get_{field_name}\" (where {field_name} is replaced with the actual name).\n",
    "        Examples would be: get_image(self, token), get_class_id(self, token), get_instances(self, token).\n",
    "        \n",
    "        A dataset loads the data from the disk as general as possible and then transformers adapt it to the needs of the neural network.\n",
    "        There are two types of transformers (which are called in the order listed here):\n",
    "        * `self.transformers = []`: These transformers are applied once on the dataset (before caching is done).\n",
    "        * `self.realtime_transformers = []`: These transformers are applied every time a sample is retrieved. (e.g. random data augmentations)\n",
    "        \n",
    "        :param config: The configuration used for your problem. (The problem parameters and train_batch_size are relevant for data loading.)\n",
    "        :param dataset_input_type: The type of the DatasetInput that the dataset outputs. This is used to automatically collect attributes from get_<attrname>.\n",
    "        :param dataset_output_type: The type of the DatasetOutput that the dataset outputs. This is used to automatically collect attributes from get_<attrname>.\n",
    "        :param cache_dir: The directory where the dataset can cache itself. Caching allows faster loading, when complex transformations are required.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.transformers = []\n",
    "        self.realtime_transformers = []\n",
    "        self._caching = False\n",
    "        self._cache_dir = cache_dir\n",
    "        self._cache_indices = {}\n",
    "        self._cached_len = -1\n",
    "        if self._cache_dir is not None:\n",
    "            self.init_caching(cache_dir)\n",
    "            self._cached_len = len(self._cache_indices)\n",
    "        self.all_sample_tokens = []\n",
    "        self.sample_tokens = None\n",
    "        self.dataset_input_type = dataset_input_type\n",
    "        self.dataset_output_type = dataset_output_type\n",
    "\n",
    "    def set_sample_token_filter(self, filter_fun):\n",
    "        \"\"\"\n",
    "        Use a filter function (lambda token: True if keep else False) to filter self.all_sample_tokens to a subset.\n",
    "\n",
    "        Use Cases:\n",
    "        * Can be used to filter out some samples.\n",
    "        * Can be used for sequence datasets to limit them to 1 sequence only.\n",
    "\n",
    "        :param filter_fun: A function that has one parameter (token) and returns true if the token should be kept and false, if the token should be removed. (If None is given, then the filter will be reset to not filtering.)\n",
    "        \"\"\"\n",
    "        if filter_fun is None:\n",
    "            self.sample_tokens = self.all_sample_tokens\n",
    "        else:\n",
    "            self.sample_tokens = filter(filter_fun, self.all_sample_tokens)\n",
    "\n",
    "    def init_caching(self, cache_dir):\n",
    "        \"\"\"\n",
    "        Initialize caching for quicker access once the data was cached once.\n",
    "        \n",
    "        The caching caches the calls to the getitem including application of regular transformers.\n",
    "        When calling this function the cache gets read if it exists or otherwise the folder is created and on first calling the getitem the item is stored.\n",
    "        \n",
    "        :param cache_dir: Directory where the cache should be stored.\n",
    "        \"\"\"\n",
    "        info(\"Init caching: {}\".format(cache_dir))\n",
    "        self._caching = True\n",
    "        self._cache_dir = cache_dir\n",
    "        # If it does not exist create the cache dir.\n",
    "        if not os.path.exists(self._cache_dir):\n",
    "            os.makedirs(self._cache_dir)\n",
    "\n",
    "        # Read all files in the folder into a dict that maps indices to filenames (for quicker access)\n",
    "        cache_files = os.listdir(self._cache_dir)\n",
    "        for cf in cache_files:\n",
    "            if cf.endswith(\".pk\"):\n",
    "                self._cache_indices[int(cf.replace(\".pk\", \"\"))] = os.path.join(self._cache_dir, cf)\n",
    "\n",
    "    def _cache(self, index: int, value) -> None:\n",
    "        fp = os.path.join(self._cache_dir, \"{:09d}.pk\".format(index))\n",
    "        with open(fp, \"wb\") as f:\n",
    "            pickle.dump(value, f)\n",
    "        self._cache_indices[index] = fp\n",
    "\n",
    "    def _fill_type_using_getters(self, namedtuple_type, sample_token):\n",
    "        data = {}\n",
    "        for k in namedtuple_type._fields:\n",
    "            getter = getattr(self, \"get_{}\".format(k), None)\n",
    "            if getter is not None:\n",
    "                data[k] = getter(sample_token)\n",
    "            else:\n",
    "                raise RuntimeError(\"Missing getter (get_{}) for dataset_input_type field: {}\".format(k, k))\n",
    "        return namedtuple_type(**data)\n",
    "\n",
    "    def getitem_by_sample_token(self, sample_token: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Gets called when an index of the dataset is accessed via dataset[idx] (aka __getitem__).\n",
    "\n",
    "        This functions returns the raw DatasetInput and DatasetOutput types, whereas the __getitem__ also calls the transformer and then returns whatever the transformer converts these types into.\n",
    "        \n",
    "        :param sample_token: The unique token that identifies a single sample from the dataset.\n",
    "        :return: A tuple of features and values for the neural network. Features must be of type DatasetInput (namedtuple) and labels of type DatasetOutput (namedtuple).\n",
    "        \"\"\"\n",
    "        dataset_input = self._fill_type_using_getters(self.dataset_input_type, sample_token)\n",
    "        dataset_output = self._fill_type_using_getters(self.dataset_output_type, sample_token)\n",
    "        return dataset_input, dataset_output\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        # If not initialized initialize\n",
    "        if self.sample_tokens is None:\n",
    "            self.set_sample_token_filter(None)\n",
    "\n",
    "        # Check if len is exceeded.\n",
    "        if index >= len(self):\n",
    "            raise IndexError()\n",
    "\n",
    "        if self._caching and index in self._cache_indices:\n",
    "            with open(self._cache_indices[index], \"rb\") as f:\n",
    "                sample = pickle.load(f)\n",
    "        else:\n",
    "            # Print index errors, they probably were an error and not intentional.\n",
    "            try:\n",
    "                sample_token = self.sample_tokens[index]\n",
    "                sample = self.getitem_by_sample_token(sample_token)\n",
    "            except IndexError as e:\n",
    "                traceback.print_exc(file=sys.stderr)\n",
    "                raise e\n",
    "\n",
    "            # Apply transforms if they are available.\n",
    "            for transform in self.transformers:\n",
    "                sample = transform(*sample)\n",
    "\n",
    "            if self._caching:\n",
    "                return self._cache(index, sample)\n",
    "\n",
    "        # Apply real time transformers after caching. Realtime is not cached\n",
    "        for transform in self.realtime_transformers:\n",
    "            sample = transform(*sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self._cached_len >= 0:\n",
    "            return self._cached_len\n",
    "        \n",
    "        # If not initialized initialize\n",
    "        if self.sample_tokens is None:\n",
    "            self.set_sample_token_filter(None)\n",
    "        \n",
    "        return len(self.sample_tokens)\n",
    "\n",
    "    @property\n",
    "    def version(self) -> str:\n",
    "        \"\"\"\n",
    "        Property that returns the version of the dataset.\n",
    "        \n",
    "        **You must not overwrite this, instead overwrite `_get_version(self) -> str` used by this property.**\n",
    "\n",
    "        :return: The version number of the dataset.\n",
    "        \"\"\"\n",
    "        version = \"{}\".format(self._get_version())\n",
    "        for transform in self.transformers:\n",
    "            version = \"{}_{}\".format(version, transform.version)\n",
    "        for transform in self.realtime_transformers:\n",
    "            version = \"{}_{}\".format(version, transform.version)\n",
    "        return version\n",
    "\n",
    "    def _get_version(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the version string of the dataset.\n",
    "        \n",
    "        **Must be overwritten by every subclass.**\n",
    "\n",
    "        :return: The version number of the dataset.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def to_dataloader(self) -> Dataloader:\n",
    "        \"\"\"\n",
    "        Converts the dataset into a babilim.data.Dataloader.\n",
    "        \n",
    "        :return: Returns a babilim dataloader object usable with the trainers.\n",
    "        \"\"\"\n",
    "        data = None\n",
    "        if babilim.is_backend(babilim.PYTORCH_BACKEND):\n",
    "            data = self.to_pytorch()\n",
    "        elif babilim.is_backend(babilim.TF_BACKEND):\n",
    "            data = self.to_keras()\n",
    "        else:\n",
    "            raise NotImplementedError(\"Other backends than pytorch and tf2 are not implemented.\")\n",
    "        return Dataloader(data, self)\n",
    "\n",
    "    def to_keras(self):\n",
    "        \"\"\"\n",
    "        Converts the dataset into a batched keras dataset.\n",
    "        \n",
    "        You can use this if you want to use a babilim dataset without babilim natively in keras.\n",
    "        \n",
    "        :return: The type will be tf.keras.Sequence.\n",
    "        \"\"\"\n",
    "        from babilim.data.keras import BatchedKerasDataset\n",
    "        return BatchedKerasDataset(self, self.config)\n",
    "\n",
    "    def to_pytorch(self):\n",
    "        \"\"\"\n",
    "        Converts the dataset into a batched pytorch dataset.\n",
    "        \n",
    "        You can use this if you want to use a babilim dataset without babilim natively in pytorch.\n",
    "        \n",
    "        :return: The type will be torch.utils.data.DataLoader.\n",
    "        \"\"\"\n",
    "        from babilim.data.pytorch import BatchedPytorchDataset\n",
    "        return BatchedPytorchDataset(self, self.config, self.config.problem_shuffle, self.config.problem_num_threads)\n",
    "\n",
    "\n",
    "    def to_tfrecord(self):\n",
    "        \"\"\"\n",
    "        Creates a tfrecord dataset from the dataset.\n",
    "\n",
    "        **Currently not implemented. Use Dataset.from_disk(...).to_keras() instead.**\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"This is not implemented yet. Use Dataset.from_disk(...).to_keras() instead.\")\n",
    "\n",
    "    def to_disk(self, cache_path: str, verbose: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Write a dataset as a cache to the disk.\n",
    " \n",
    "        :param cache_path: The path where the cache should be written.\n",
    "        :param verbose: If info on progress should be printed, defaults to True.\n",
    "        \"\"\"\n",
    "        self.init_caching(cache_path)\n",
    "        if verbose:\n",
    "            info(\"Caching dataset to {}\".format(cache_path))\n",
    "        N = len(self)\n",
    "        for i, _ in enumerate(self):\n",
    "            if verbose:\n",
    "                status(\"{}/{}\".format(i, N), end=\"\")\n",
    "        \n",
    "        if verbose:\n",
    "            info(\"\")\n",
    "            info(\"Caching done.\")\n",
    " \n",
    "    @staticmethod\n",
    "    def from_disk(config: Config, cache_path: str) -> 'Dataset':\n",
    "        \"\"\"\n",
    "        Create a dataset from a cache on disk.\n",
    "\n",
    "        :param config: The configuration for the dataset.\n",
    "        :param cache_path: The path to the cache.\n",
    "        :param version: The version of the dataset that should be loaded.\n",
    "        :return: A Dataset object that represents the data that has been passed to \"to_disk\" when creating the cache.\n",
    "        \"\"\"\n",
    "        return Dataset(config, cache_dir=cache_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

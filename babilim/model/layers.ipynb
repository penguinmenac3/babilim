{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# babilim.model.layers\n",
    "\n",
    "> Here you can find all layers implemented in babilim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Tuple, Iterable, Sequence, List, Dict, Optional, Union, Any\n",
    "from babilim import PYTORCH_BACKEND, TF_BACKEND, is_backend, get_backend\n",
    "from babilim.core import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various (Conv, Linear, BatchNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Flatten() -> Module:\n",
    "    \"\"\"\n",
    "    Flatten a feature map into a linearized tensor.\n",
    "    \n",
    "    This is usefull after the convolution layers before the dense layers. The (B, W, H, C) tensor gets converted ot a (B, N) tensor.\n",
    "    \n",
    "    :return: A module implementing the flatten layer.\n",
    "    \"\"\"\n",
    "    if is_backend(PYTORCH_BACKEND):\n",
    "        from babilim.model.modules.pt.flatten import Flatten as _Flatten\n",
    "        return _Flatten()\n",
    "    elif is_backend(TF_BACKEND):\n",
    "        from babilim.model.modules.tf.flatten import Flatten as _Flatten\n",
    "        return _Flatten()\n",
    "    else:\n",
    "        raise NotImplementedError(\"The backend {} is not implemented by this modules.\".format(get_backend()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Linear(out_features: int, activation=None) -> Module:\n",
    "    \"\"\"\n",
    "    A simple linear layer.\n",
    "\n",
    "    It computes Wx+b with no activation funciton.\n",
    "    \n",
    "    :param out_features: The number of output features.\n",
    "    :param activation: The activation function that should be added after the linear layer.\n",
    "    :return: A module implementing the linear layer.\n",
    "    \"\"\"\n",
    "    if is_backend(PYTORCH_BACKEND):\n",
    "        from babilim.model.modules.pt.linear import Linear as _Linear\n",
    "        return _Linear(out_features, activation=activation)\n",
    "    elif is_backend(TF_BACKEND):\n",
    "        from babilim.model.modules.tf.linear import Linear as _Linear\n",
    "        return _Linear(out_features, activation=activation)\n",
    "    else:\n",
    "        raise NotImplementedError(\"The backend {} is not implemented by this modules.\".format(get_backend()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Dense(out_features: int, activation=None) -> Module:\n",
    "    \"\"\"\n",
    "    A simple dense layer (alias for Linear Layer).\n",
    "\n",
    "    It computes Wx+b with no activation funciton.\n",
    "    \n",
    "    :param out_features: The number of output features.\n",
    "    :param activation: The activation function that should be added after the dense layer.\n",
    "    :return: A module implementing the dense layer.\n",
    "    \"\"\"\n",
    "    return Linear(out_features, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Conv1D(filters: int, kernel_size: int, padding: Optional[str] = None, strides: int = 1, dilation_rate: int = 1, kernel_initializer: Optional[Any] = None, activation=None) -> Module:\n",
    "    \"\"\"\n",
    "    A 1d convolution layer.\n",
    "    \n",
    "    :param filters: The number of filters in the convolution. Defines the number of output channels.\n",
    "    :param kernel_size: The kernel size of the convolution. Defines the area over which is convolved. Typically 1, 3 or 5 are recommended.\n",
    "    :param padding: What type of padding should be applied. The string \"none\" means no padding is applied, None or \"same\" means the input is padded in a way that the output stays the same size if no stride is applied.\n",
    "    :param stride: The offset between two convolutions that are applied. Typically 1. Stride affects also the resolution of the output feature map. A stride 2 halves the resolution, since convolutions are only applied every odd pixel.\n",
    "    :param dilation_rate: The dilation rate for a convolution.\n",
    "    :param kernel_initializer: A kernel initializer function. By default orthonormal weight initialization is used.\n",
    "    :param activation: The activation function that should be added after the dense layer.\n",
    "    :return: A module implementing the convolution layer.\n",
    "    \"\"\"\n",
    "    if is_backend(PYTORCH_BACKEND):\n",
    "        from babilim.model.modules.pt.conv import Conv1D as _Conv1D\n",
    "        return _Conv1D(filters, kernel_size, padding, strides, dilation_rate, kernel_initializer, activation=activation)\n",
    "    elif is_backend(TF_BACKEND):\n",
    "        from babilim.model.modules.tf.conv import Conv1D as _Conv1D\n",
    "        return _Conv1D(filters, kernel_size, padding, strides, dilation_rate, kernel_initializer, activation=activation)\n",
    "    else:\n",
    "        raise NotImplementedError(\"The backend {} is not implemented by this modules.\".format(get_backend()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Conv2D(filters: int, kernel_size: Tuple[int, int], padding: Optional[str] = None, strides: Tuple[int, int] = (1, 1), dilation_rate: Tuple[int, int] = (1, 1), kernel_initializer: Optional[Any] = None, activation=None) -> Module:\n",
    "    \"\"\"\n",
    "    A 2d convolution layer.\n",
    "    \n",
    "    :param filters: The number of filters in the convolution. Defines the number of output channels.\n",
    "    :param kernel_size: The kernel size of the convolution. Defines the area over which is convolved. Typically (1,1) (3,3) or (5,5) are recommended.\n",
    "    :param padding: What type of padding should be applied. The string \"none\" means no padding is applied, None or \"same\" means the input is padded in a way that the output stays the same size if no stride is applied.\n",
    "    :param stride: The offset between two convolutions that are applied. Typically (1, 1). Stride affects also the resolution of the output feature map. A stride 2 halves the resolution, since convolutions are only applied every odd pixel.\n",
    "    :param dilation_rate: The dilation rate for a convolution.\n",
    "    :param kernel_initializer: A kernel initializer function. By default orthonormal weight initialization is used.\n",
    "    :param activation: The activation function that should be added after the dense layer.\n",
    "    :return: A module implementing the convolution layer.\n",
    "    \"\"\"\n",
    "    if is_backend(PYTORCH_BACKEND):\n",
    "        from babilim.model.modules.pt.conv import Conv2D as _Conv2D\n",
    "        return _Conv2D(filters, kernel_size, padding, strides, dilation_rate, kernel_initializer, activation=activation)\n",
    "    elif is_backend(TF_BACKEND):\n",
    "        from babilim.model.modules.tf.conv import Conv2D as _Conv2D\n",
    "        return _Conv2D(filters, kernel_size, padding, strides, dilation_rate, kernel_initializer, activation=activation)\n",
    "    else:\n",
    "        raise NotImplementedError(\"The backend {} is not implemented by this modules.\".format(get_backend()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def BatchNormalization() -> Module:\n",
    "    \"\"\"\n",
    "    A batch normalization layer.\n",
    "    \n",
    "    :return: A module implementing the batch normalization layer.\n",
    "    \"\"\"\n",
    "    if is_backend(PYTORCH_BACKEND):\n",
    "        from babilim.model.modules.pt.batch_normalization import BatchNormalization as _BatchNormalization\n",
    "        return _BatchNormalization()\n",
    "    elif is_backend(TF_BACKEND):\n",
    "        from babilim.model.modules.tf.batch_normalization import BatchNormalization as _BatchNormalization\n",
    "        return _BatchNormalization()\n",
    "    else:\n",
    "        raise NotImplementedError(\"The backend {} is not implemented by this modules.\".format(get_backend()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def GlobalMaxPooling2D() -> Module:\n",
    "    \"\"\"\n",
    "    A global max pooling layer.\n",
    "    \n",
    "    This computes the global max in W, H dimension, so that the result is of shape (B, C).\n",
    "    \n",
    "    :return: A module implementing the global max pooling 2d.\n",
    "    \"\"\"\n",
    "    if is_backend(PYTORCH_BACKEND):\n",
    "        from babilim.model.modules.pt.pooling import GlobalMaxPooling2D as _GlobalMaxPooling2D\n",
    "        return _GlobalMaxPooling2D()\n",
    "    elif is_backend(TF_BACKEND):\n",
    "        from babilim.model.modules.tf.pooling import GlobalMaxPooling2D as _GlobalMaxPooling2D\n",
    "        return _GlobalMaxPooling2D()\n",
    "    else:\n",
    "        raise NotImplementedError(\"The backend {} is not implemented by this modules.\".format(get_backend()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def GlobalMaxPooling1D() -> Module:\n",
    "    \"\"\"\n",
    "    A global max pooling layer.\n",
    "    \n",
    "    This computes the global max in N dimension (B, N, C), so that the result is of shape (B, C).\n",
    "    \n",
    "    :return: A module implementing the global max pooling 1d.\n",
    "    \"\"\"\n",
    "    if is_backend(PYTORCH_BACKEND):\n",
    "        from babilim.model.modules.pt.pooling import GlobalMaxPooling1D as _GlobalMaxPooling1D\n",
    "        return _GlobalMaxPooling1D()\n",
    "    elif is_backend(TF_BACKEND):\n",
    "        from babilim.model.modules.tf.pooling import GlobalMaxPooling1D as _GlobalMaxPooling1D\n",
    "        return _GlobalMaxPooling1D()\n",
    "    else:\n",
    "        raise NotImplementedError(\"The backend {} is not implemented by this modules.\".format(get_backend()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MaxPooling2D() -> Module:\n",
    "    \"\"\"\n",
    "    A 2x2 max pooling layer.\n",
    "    \n",
    "    Computes the max of a 2x2 region with stride 2.\n",
    "    This halves the feature map size.\n",
    "    \n",
    "    :return A module implementing the 2x2 max pooling.\n",
    "    \"\"\"\n",
    "    if is_backend(PYTORCH_BACKEND):\n",
    "        from babilim.model.modules.pt.pooling import MaxPooling2D as _MaxPooling2D\n",
    "        return _MaxPooling2D()\n",
    "    elif is_backend(TF_BACKEND):\n",
    "        from babilim.model.modules.tf.pooling import MaxPooling2D as _MaxPooling2D\n",
    "        return _MaxPooling2D()\n",
    "    else:\n",
    "        raise NotImplementedError(\"The backend {} is not implemented by this modules.\".format(get_backend()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MaxPooling1D() -> Module:\n",
    "    \"\"\"\n",
    "    A max pooling layer.\n",
    "    \n",
    "    Computes the max of a 2 region with stride 2.\n",
    "    This halves the feature map size.\n",
    "    \n",
    "    :return A module implementing the 2 max pooling.\n",
    "    \"\"\"\n",
    "    if is_backend(PYTORCH_BACKEND):\n",
    "        from babilim.model.modules.pt.pooling import MaxPooling1D as _MaxPooling1D\n",
    "        return _MaxPooling1D()\n",
    "    elif is_backend(TF_BACKEND):\n",
    "        from babilim.model.modules.tf.pooling import MaxPooling1D as _MaxPooling1D\n",
    "        return _MaxPooling1D()\n",
    "    else:\n",
    "        raise NotImplementedError(\"The backend {} is not implemented by this modules.\".format(get_backend()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def GlobalAveragePooling2D() -> Module:\n",
    "    \"\"\"\n",
    "    A global average pooling layer.\n",
    "    \n",
    "    This computes the global average in W, H dimension, so that the result is of shape (B, C).\n",
    "    \n",
    "    :return: A module implementing the global average pooling 2d.\n",
    "    \"\"\"\n",
    "    if is_backend(PYTORCH_BACKEND):\n",
    "        from babilim.model.modules.pt.pooling import GlobalAveragePooling2D as _GlobalAveragePooling2D\n",
    "        return _GlobalAveragePooling2D()\n",
    "    elif is_backend(TF_BACKEND):\n",
    "        from babilim.model.modules.tf.pooling import GlobalAveragePooling2D as _GlobalAveragePooling2D\n",
    "        return _GlobalAveragePooling2D()\n",
    "    else:\n",
    "        raise NotImplementedError(\"The backend {} is not implemented by this modules.\".format(get_backend()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def GlobalAveragePooling1D() -> Module:\n",
    "    \"\"\"\n",
    "    A global average pooling layer.\n",
    "    \n",
    "    This computes the global average in N dimension (B, N, C), so that the result is of shape (B, C).\n",
    "    \n",
    "    :return: A module implementing the global average pooling 1d.\n",
    "    \"\"\"\n",
    "    if is_backend(PYTORCH_BACKEND):\n",
    "        from babilim.model.modules.pt.pooling import GlobalAveragePooling1D as _GlobalAveragePooling1D\n",
    "        return _GlobalAveragePooling1D()\n",
    "    elif is_backend(TF_BACKEND):\n",
    "        from babilim.model.modules.tf.pooling import GlobalAveragePooling1D as _GlobalAveragePooling1D\n",
    "        return _GlobalAveragePooling1D()\n",
    "    else:\n",
    "        raise NotImplementedError(\"The backend {} is not implemented by this modules.\".format(get_backend()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Activation(activation: str) -> Module:\n",
    "    \"\"\"\n",
    "    Supports the activation functions.\n",
    "    \n",
    "    :param activation: A string specifying the activation function to use. (Only \"relu\" and None supported yet.)\n",
    "    \"\"\"\n",
    "    if is_backend(PYTORCH_BACKEND):\n",
    "        from babilim.model.modules.pt.activation import Activation as _Activation\n",
    "        return _Activation(activation)\n",
    "    elif is_backend(TF_BACKEND):\n",
    "        from babilim.model.modules.tf.activation import Activation as _Activation\n",
    "        return _Activation(activation)\n",
    "    else:\n",
    "        raise NotImplementedError(\"The backend {} is not implemented by this modules.\".format(get_backend()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composite Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Lambda(native_module, to_gpu=True) -> Module:\n",
    "    \"\"\"\n",
    "    Wrap a natively implemented layer into a babilim layer.\n",
    "    \n",
    "    This can be used to implement layers that are missing in babilim in an easy way.\n",
    "    \n",
    "    ```\n",
    "    my_max = Lambda(tf.max)\n",
    "    ```\n",
    "    \n",
    "    :param native_module: The native pytorch/tensorflow module that should be wrapped.\n",
    "    :param to_gpu: (Optional) True if the module should be automatically be moved to the gpu. (default: True)\n",
    "    \"\"\"\n",
    "    from babilim.model.modules.common.module_wrapper import Lambda as _Lambda\n",
    "    return _Lambda(native_module, to_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Sequential(*layers) -> Module:\n",
    "    \"\"\"\n",
    "    Create a module which is a sequential order of other layers.\n",
    "    \n",
    "    Runs the layers in order.\n",
    "    \n",
    "    ```python\n",
    "    my_seq = Sequential(layer1, layer2, layer3)\n",
    "    ```\n",
    "    \n",
    "    :param layers: All ordered parameters are used as layers.\n",
    "    \"\"\"\n",
    "    from babilim.model.modules.common.sequential import Sequential as _Sequential\n",
    "    return _Sequential(*layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# babilim.model.layers.native_wrapper\n",
    "\n",
    "> Wrap a layer, function or model from native into babilim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Any\n",
    "\n",
    "import babilim\n",
    "from babilim.core.logging import info\n",
    "from babilim.core import Tensor, RunOnlyOnce\n",
    "from babilim.core.module import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _isnamedtupleinstance(x):\n",
    "    t = type(x)\n",
    "    b = t.__bases__\n",
    "    if len(b) != 1 or b[0] != tuple: return False\n",
    "    f = getattr(t, '_fields', None)\n",
    "    if not isinstance(f, tuple): return False\n",
    "    return all(type(n)==str for n in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Lambda(Module):\n",
    "    def __init__(self, native_module, to_gpu=True):\n",
    "        \"\"\"\n",
    "        Wrap a natively implemented layer into a babilim layer.\n",
    "    \n",
    "        This can be used to implement layers that are missing in babilim in an easy way.\n",
    "        \n",
    "        ```\n",
    "        my_lambda = Lambda(tf.max)\n",
    "        ```\n",
    "        \n",
    "        :param native_module: The native pytorch/tensorflow module that should be wrapped.\n",
    "        :param to_gpu: (Optional) True if the module should be automatically be moved to the gpu. (default: True)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.native_module = native_module\n",
    "\n",
    "    def _auto_device(self):\n",
    "        if babilim.is_backend(babilim.PYTORCH_BACKEND):\n",
    "            import torch\n",
    "            self.native_module = self.native_module.to(torch.device(self.device))\n",
    "            return self\n",
    "    \n",
    "    @RunOnlyOnce\n",
    "    def build(self, *args, **kwargs) -> None:\n",
    "        self._auto_device()\n",
    "        build = getattr(self.native_module, \"build\", None)\n",
    "        if callable(build):\n",
    "            # Unwrap arguments\n",
    "            args = [feature.native for feature in args]\n",
    "            kwargs = {k: kwargs[k].native for k in kwargs}\n",
    "\n",
    "            # Call the build\n",
    "            build(*args, **kwargs)\n",
    "\n",
    "    def call(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Do not call this directly, use `__call__`:\n",
    "        ```\n",
    "        my_lambda(*args, **kwargs)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        # Unwrap arguments\n",
    "        args = [feature.native for feature in args]\n",
    "        kwargs = {k: kwargs[k].native for k in kwargs}\n",
    "\n",
    "        # call function\n",
    "        result = self.native_module(*args, **kwargs)\n",
    "        # Wrap results\n",
    "        if _isnamedtupleinstance(result):\n",
    "            result_raw = result._asdict()\n",
    "            result_raw = {k: Tensor(data=result_raw[k], trainable=True) for k in result_raw}\n",
    "            return type(result)(**result_raw)\n",
    "        elif isinstance(result, dict):\n",
    "            result = {k: Tensor(data=result[k], trainable=True) for k in result}\n",
    "        elif isinstance(result, list):\n",
    "            result = [Tensor(data=res, trainable=True) for res in result]\n",
    "        else:\n",
    "            result = Tensor(data=result, trainable=True)\n",
    "        return result\n",
    "\n",
    "    def eval(self):\n",
    "        self.train(False)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        train_fn = getattr(self.native_module, \"train\", None)\n",
    "        if callable(train_fn):\n",
    "            train_fn(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(10, 8, 8, 3)\n(10, 3, 8, 3)\ndict_keys(['/native_module/weight', '/native_module/bias'])\n"
    }
   ],
   "source": [
    "from babilim.core.tensor import Tensor\n",
    "import numpy as np\n",
    "from torch.nn import Conv1d\n",
    "\n",
    "native_conv = Conv1d(in_channels=8, out_channels=3, kernel_size=(1,1))\n",
    "my_lambda = Lambda(native_conv)\n",
    "tensor = Tensor(data=np.zeros((10,8,8,3), dtype=np.float32), trainable=False)\n",
    "\n",
    "print(tensor.shape)\n",
    "result = my_lambda(tensor)\n",
    "print(result.shape)\n",
    "print(my_lambda.named_trainable_variables.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python38364bitbasecondae7cd72b7144542bdae788b1dbf27e222"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
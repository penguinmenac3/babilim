{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# babilim.training.losses\n",
    "\n",
    "> A package containing all losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import defaultdict\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import babilim\n",
    "from babilim.core.itensor import ITensor\n",
    "from babilim.core.logging import info\n",
    "from babilim.core.tensor import Tensor\n",
    "from babilim.core.module import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Loss(Module):\n",
    "    def __init__(self, log_std=False, log_min=False, log_max=False, reduction: str = \"mean\"):\n",
    "        \"\"\"\n",
    "        A loss is a statefull object which computes the difference between the prediction and the target.\n",
    "        \n",
    "        :param log_std: When true the loss will log its standard deviation. (default: False)\n",
    "        :param log_min: When true the loss will log its minimum values. (default: False)\n",
    "        :param log_max: When true the loss will log its maximal values. (default: False)\n",
    "        :param reduction: Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Default: 'mean'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._accumulators = defaultdict(list)\n",
    "        self._log_std = log_std\n",
    "        self._log_min = log_min\n",
    "        self._log_max = log_max\n",
    "        self.reduction = reduction\n",
    "        if reduction not in [\"none\", \"mean\", \"sum\"]:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def call(self, y_pred: Any, y_true: Any) -> ITensor:\n",
    "        \"\"\"\n",
    "        Implement a loss function between preds and true outputs.\n",
    "        \n",
    "        **DO NOT**:\n",
    "        * Overwrite this function (overwrite `self.loss(...)` instead)\n",
    "        * Call this function (call the module instead `self(y_pred, y_true)`)\n",
    "\n",
    "        Arguments:\n",
    "        :param y_pred: The predictions of the network. Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        :param y_true: The desired outputs of the network (labels). Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        \"\"\"\n",
    "        loss = self.loss(y_pred, y_true)\n",
    "        if loss.is_nan().any():\n",
    "            raise ValueError(\"Loss is nan. Loss value: {}\".format(loss))\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "        return loss\n",
    "\n",
    "    def loss(self, y_pred: Any, y_true: Any) -> ITensor:\n",
    "        \"\"\"\n",
    "        Implement a loss function between preds and true outputs.\n",
    "        \n",
    "        **`loss` must be overwritten by subclasses.**\n",
    "        \n",
    "        **DO NOT**:\n",
    "        * Call this function (call the module instead `self(y_pred, y_true)`)\n",
    "\n",
    "        Arguments:\n",
    "        :param y_pred: The predictions of the network. Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        :param y_true: The desired outputs of the network (labels). Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Every loss must implement the call method.\")\n",
    "\n",
    "    def log(self, name: str, value: ITensor) -> None:\n",
    "        \"\"\"\n",
    "        Log a tensor under a name.\n",
    "        \n",
    "        These logged values then can be used for example by tensorboard loggers.\n",
    "        \n",
    "        :param name: The name under which to log the tensor.\n",
    "        :param value: The tensor that should be logged.\n",
    "        \"\"\"\n",
    "        val = value.numpy()\n",
    "        if len(val.shape) > 0:\n",
    "            self._accumulators[name].append(val)\n",
    "        else:\n",
    "            self._accumulators[name].append(np.array([val]))\n",
    "\n",
    "    def reset_avg(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the accumulation of tensors in the logging.\n",
    "        \n",
    "        Should only be called by a tensorboard logger.\n",
    "        \"\"\"\n",
    "        self._accumulators = defaultdict(list)\n",
    "\n",
    "    def summary(self, samples_seen, summary_writer=None) -> None:\n",
    "        \"\"\"\n",
    "        Write a summary of the accumulated logs into tensorboard.\n",
    "        \n",
    "        :param samples_seen: The number of samples the training algorithm has seen so far (not iterations!).\n",
    "            This is used for the x axis in the plot. If you use the samples seen it is independant of the batch size.\n",
    "            If the network was trained for 4 batches with 32 batch size or for 32 batches with 4 batchsize does not matter.\n",
    "        :param summary_writer: The summary writer to use for writing the summary. If none is provided it will use the tensorflow default.\n",
    "        \"\"\"\n",
    "        if summary_writer is not None:\n",
    "            for k in self._accumulators:\n",
    "                if not self._accumulators[k]:\n",
    "                    continue\n",
    "                combined = np.concatenate(self._accumulators[k], axis=0)\n",
    "                summary_writer.add_scalar(\"{}\".format(k), combined.mean(), global_step=samples_seen)\n",
    "                if self._log_std:\n",
    "                    summary_writer.add_scalar(\"{}_std\".format(k), combined.std(), global_step=samples_seen)\n",
    "                if self._log_min:\n",
    "                    summary_writer.add_scalar(\"{}_min\".format(k), combined.min(), global_step=samples_seen)\n",
    "                if self._log_max:\n",
    "                    summary_writer.add_scalar(\"{}_max\".format(k), combined.max(), global_step=samples_seen)\n",
    "        else:\n",
    "            import tensorflow as tf\n",
    "            for k in self._accumulators:\n",
    "                if not self._accumulators[k]:\n",
    "                    continue\n",
    "                combined = np.concatenate(self._accumulators[k], axis=0)\n",
    "                tf.summary.scalar(\"{}\".format(k), combined.mean(), step=samples_seen)\n",
    "                if self._log_std:\n",
    "                    tf.summary.scalar(\"{}_std\".format(k), combined.std(), step=samples_seen)\n",
    "                if self._log_min:\n",
    "                    tf.summary.scalar(\"{}_min\".format(k), combined.min(), step=samples_seen)\n",
    "                if self._log_max:\n",
    "                    tf.summary.scalar(\"{}_max\".format(k), combined.max(), step=samples_seen)\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        \"\"\"\n",
    "        Get the average of the loged values.\n",
    "        \n",
    "        This is helpfull to print values that are more stable than values from a single iteration.\n",
    "        \"\"\"\n",
    "        avgs = {}\n",
    "        for k in self._accumulators:\n",
    "            if not self._accumulators[k]:\n",
    "                continue\n",
    "            combined = np.concatenate(self._accumulators[k], axis=0)\n",
    "            avgs[k] = combined.mean()\n",
    "        return avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NativeLossWrapper(Loss):\n",
    "    def __init__(self, loss, log_std=False, log_min=False, log_max=False, reduction: str = \"mean\"):\n",
    "        \"\"\"\n",
    "        Wrap a native loss as a babilim loss.\n",
    "\n",
    "        The wrapped object must have the following signature:\n",
    "\n",
    "        ```python\n",
    "        Callable(y_pred, y_true, log_val) -> Tensor\n",
    "        ```\n",
    "\n",
    "        where log_val will be a function which can be used for logging scalar tensors/values.\n",
    "\n",
    "        :param loss: The loss that should be wrapped.\n",
    "        :param log_std: When true the loss will log its standard deviation. (default: False)\n",
    "        :param log_min: When true the loss will log its minimum values. (default: False)\n",
    "        :param log_max: When true the loss will log its maximal values. (default: False)\n",
    "        :param reduction: Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Default: 'mean'.\n",
    "        \"\"\"\n",
    "        super().__init__(log_std=log_std, log_min=log_min, log_max=log_max, reduction=reduction)\n",
    "        self.native_loss = loss\n",
    "        self._auto_device()\n",
    "\n",
    "    def _auto_device(self):\n",
    "        if babilim.is_backend(babilim.PYTORCH_BACKEND):\n",
    "            import torch\n",
    "            self.native_loss = self.native_loss.to(torch.device(self.device))\n",
    "            return self\n",
    "\n",
    "    def loss(self, y_pred: Any, y_true: Any) -> ITensor:\n",
    "        \"\"\"\n",
    "        Compute the loss using the native loss function provided in the constructor.\n",
    "        \n",
    "        :param y_pred: The predictions of the network. Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        :param y_true: The desired outputs of the network (labels). Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        \"\"\"\n",
    "        # Unwrap arguments\n",
    "        tmp = y_true._asdict()\n",
    "        y_true_tmp = {k: tmp[k].native for k in tmp}\n",
    "        y_true = type(y_true)(**y_true_tmp)\n",
    "\n",
    "        tmp = y_pred._asdict()\n",
    "        y_pred_tmp = {k: tmp[k].native for k in tmp}\n",
    "        y_pred = type(y_pred)(**y_pred_tmp)\n",
    "\n",
    "        # call function\n",
    "        result = self.native_loss(y_pred=y_pred, y_true=y_true,\n",
    "                           log_val=lambda name, tensor: self.log(name, Tensor(data=tensor, trainable=True)))\n",
    "\n",
    "        return Tensor(data=result, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SparseCrossEntropyLossFromLogits(Loss):\n",
    "    def __init__(self, log_std=False, log_min=False, log_max=False, reduction: str = \"mean\"):\n",
    "        \"\"\"\n",
    "        Compute a sparse cross entropy.\n",
    "        \n",
    "        This means that the preds are logits and the targets are not one hot encoded.\n",
    "        \n",
    "        :param log_std: When true the loss will log its standard deviation. (default: False)\n",
    "        :param log_min: When true the loss will log its minimum values. (default: False)\n",
    "        :param log_max: When true the loss will log its maximal values. (default: False)\n",
    "        :param reduction: Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Default: 'mean'.\n",
    "        \"\"\"\n",
    "        super().__init__(log_std=log_std, log_min=log_min, log_max=log_min, reduction=reduction)\n",
    "        if babilim.is_backend(babilim.PYTORCH_BACKEND):\n",
    "            from torch.nn import CrossEntropyLoss\n",
    "            self.loss_fun = CrossEntropyLoss(reduction=\"none\")\n",
    "        else:\n",
    "            from tensorflow.nn import sparse_softmax_cross_entropy_with_logits\n",
    "            self.loss_fun = sparse_softmax_cross_entropy_with_logits\n",
    "\n",
    "    def loss(self, y_pred: ITensor, y_true: ITensor) -> ITensor:\n",
    "        \"\"\"\n",
    "        Compute the sparse cross entropy assuming y_pred to be logits.\n",
    "        \n",
    "        :param y_pred: The predictions of the network. Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        :param y_true: The desired outputs of the network (labels). Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        \"\"\"\n",
    "        y_true = y_true.cast(\"int64\")\n",
    "        if babilim.is_backend(babilim.PYTORCH_BACKEND):\n",
    "            return Tensor(data=self.loss_fun(y_pred.native, y_true.native), trainable=True)\n",
    "        else:\n",
    "            return Tensor(data=self.loss_fun(labels=y_true.native, logits=y_pred.native), trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BinaryCrossEntropyLossFromLogits(Loss):\n",
    "    def __init__(self, log_std=False, log_min=False, log_max=False, reduction: str = \"mean\"):\n",
    "        \"\"\"\n",
    "        Compute a binary cross entropy.\n",
    "        \n",
    "        This means that the preds are logits and the targets are a binary (1 or 0) tensor of same shape as logits.\n",
    "\n",
    "        :param log_std: When true the loss will log its standard deviation. (default: False)\n",
    "        :param log_min: When true the loss will log its minimum values. (default: False)\n",
    "        :param log_max: When true the loss will log its maximal values. (default: False)\n",
    "        :param reduction: Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Default: 'mean'.\n",
    "        \"\"\"\n",
    "        super().__init__(log_std=log_std, log_min=log_min, log_max=log_min, reduction=reduction)\n",
    "        if babilim.is_backend(babilim.PYTORCH_BACKEND):\n",
    "            from torch.nn import BCEWithLogitsLoss\n",
    "            self.loss_fun = BCEWithLogitsLoss(reduction=\"none\")\n",
    "        else:\n",
    "            from tensorflow.nn import sigmoid_cross_entropy_with_logits\n",
    "            self.loss_fun = sigmoid_cross_entropy_with_logits\n",
    "\n",
    "    def loss(self, y_pred: ITensor, y_true: ITensor) -> ITensor:\n",
    "        \"\"\"\n",
    "        Compute the sparse cross entropy assuming y_pred to be logits.\n",
    "        \n",
    "        :param y_pred: The predictions of the network. Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        :param y_true: The desired outputs of the network (labels). Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        \"\"\"\n",
    "        if babilim.is_backend(babilim.PYTORCH_BACKEND):\n",
    "            return Tensor(data=self.loss_fun(y_pred.native, y_true.native), trainable=True)\n",
    "        else:\n",
    "            return Tensor(data=self.loss_fun(labels=y_true.native, logits=y_pred.native), trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SmoothL1Loss(Loss):\n",
    "    def __init__(self, log_std=False, log_min=False, log_max=False, reduction: str = \"mean\"):\n",
    "        \"\"\"\n",
    "        Compute a binary cross entropy.\n",
    "        \n",
    "        This means that the preds are logits and the targets are a binary (1 or 0) tensor of same shape as logits.\n",
    "\n",
    "        :param log_std: When true the loss will log its standard deviation. (default: False)\n",
    "        :param log_min: When true the loss will log its minimum values. (default: False)\n",
    "        :param log_max: When true the loss will log its maximal values. (default: False)\n",
    "        :param reduction: Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Default: 'mean'.\n",
    "        \"\"\"\n",
    "        super().__init__(log_std=log_std, log_min=log_min, log_max=log_min, reduction=reduction)\n",
    "        if babilim.is_backend(babilim.PYTORCH_BACKEND):\n",
    "            from torch.nn import SmoothL1Loss\n",
    "            self.loss_fun = SmoothL1Loss(reduction=\"none\")\n",
    "        else:\n",
    "            from tensorflow.keras.losses import huber\n",
    "            self.loss_fun = huber\n",
    "            self.delta = 1.0\n",
    "\n",
    "    def loss(self, y_pred: ITensor, y_true: ITensor) -> ITensor:\n",
    "        \"\"\"\n",
    "        Compute the sparse cross entropy assuming y_pred to be logits.\n",
    "        \n",
    "        :param y_pred: The predictions of the network. Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        :param y_true: The desired outputs of the network (labels). Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        \"\"\"\n",
    "        if babilim.is_backend(babilim.PYTORCH_BACKEND):\n",
    "            return Tensor(data=self.loss_fun(y_pred.native, y_true.native), trainable=True)\n",
    "        else:\n",
    "            return Tensor(data=self.loss_fun(labels=y_true.native, logits=y_pred.native, delta=self.delta), trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MeanSquaredError(Loss):\n",
    "    def __init__(self, log_std=False, log_min=False, log_max=False, reduction: str = \"mean\"):\n",
    "        \"\"\"\n",
    "        Compute the mean squared error.\n",
    "        \n",
    "        :param log_std: When true the loss will log its standard deviation. (default: False)\n",
    "        :param log_min: When true the loss will log its minimum values. (default: False)\n",
    "        :param log_max: When true the loss will log its maximal values. (default: False)\n",
    "        :param reduction: Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Default: 'mean'.\n",
    "        \"\"\"\n",
    "        super().__init__(log_std=log_std, log_min=log_min, log_max=log_min, reduction=reduction)\n",
    "    \n",
    "    def loss(self, y_pred: ITensor, y_true: ITensor, axis: int=-1) -> ITensor:\n",
    "        \"\"\"\n",
    "        Compute the mean squared error.\n",
    "        \n",
    "        :param y_pred: The predictions of the network. Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        :param y_true: The desired outputs of the network (labels). Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        :param axis: (Optional) The axis along which to compute the mean squared error.\n",
    "        \"\"\"\n",
    "        return ((y_pred - y_true) ** 2).mean(axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SparseCategoricalAccuracy(Loss):\n",
    "    def __init__(self, log_std=False, log_min=False, log_max=False, reduction: str = \"mean\"):\n",
    "        \"\"\"\n",
    "        Compute the sparse mean squared error.\n",
    "        \n",
    "        Sparse means that the targets are not one hot encoded.\n",
    "        \n",
    "        :param log_std: When true the loss will log its standard deviation. (default: False)\n",
    "        :param log_min: When true the loss will log its minimum values. (default: False)\n",
    "        :param log_max: When true the loss will log its maximal values. (default: False)\n",
    "        :param reduction: Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Default: 'mean'.\n",
    "        \"\"\"\n",
    "        super().__init__(log_std=log_std, log_min=log_min, log_max=log_min, reduction=reduction)\n",
    "\n",
    "    def loss(self, y_pred: ITensor, y_true: ITensor, axis: int=-1) -> ITensor:\n",
    "        \"\"\"\n",
    "        Compute the sparse categorical accuracy.\n",
    "        \n",
    "        :param y_pred: The predictions of the network. Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        :param y_true: The desired outputs of the network (labels). Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        :param axis: (Optional) The axis along which to compute the sparse categorical accuracy.\n",
    "        \"\"\"\n",
    "        pred_class = y_pred.argmax(axis=axis)\n",
    "        true_class = y_true.cast(\"int64\")\n",
    "        correct_predictions = pred_class == true_class\n",
    "        return correct_predictions.cast(\"float32\").mean(axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NaNMaskedLoss(Loss):\n",
    "    def __init__(self, loss, log_std=False, log_min=False, log_max=False):\n",
    "        \"\"\"\n",
    "        Compute a sparse cross entropy.\n",
    "        \n",
    "        This means that the preds are logits and the targets are not one hot encoded.\n",
    "        \n",
    "        :param loss: The loss that should be wrapped and only applied on non nan values.\n",
    "        :param log_std: When true the loss will log its standard deviation. (default: False)\n",
    "        :param log_min: When true the loss will log its minimum values. (default: False)\n",
    "        :param log_max: When true the loss will log its maximal values. (default: False)\n",
    "        \"\"\"\n",
    "        super().__init__(log_std=log_std, log_min=log_min, log_max=log_min, reduction=\"none\")\n",
    "        self.wrapped_loss = loss\n",
    "        self.zero = Tensor(data=np.array(0), trainable=False)\n",
    "\n",
    "    def loss(self, y_pred: ITensor, y_true: ITensor) -> ITensor:\n",
    "        \"\"\"\n",
    "        Compute the loss given in the constructor only on values where the GT is not NaN.\n",
    "        \n",
    "        :param y_pred: The predictions of the network. Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        :param y_true: The desired outputs of the network (labels). Either a NamedTuple pointing at ITensors or a Dict or Tuple of ITensors.\n",
    "        \"\"\"\n",
    "        binary_mask = (~y_true.is_nan())\n",
    "        mask = binary_mask.cast(\"float32\")\n",
    "        masked_y_true = (y_true * mask)[binary_mask]\n",
    "\n",
    "        if y_pred.shape[-1] != binary_mask.shape[-1] and binary_mask.shape[-1] == 1:\n",
    "            new_shape = binary_mask.shape[:-1]\n",
    "            binary_mask = binary_mask.reshape(new_shape)\n",
    "        masked_y_pred = (y_pred * mask)[binary_mask]\n",
    "        \n",
    "        if masked_y_pred.shape[0] > 0:\n",
    "            loss = self.wrapped_loss(masked_y_pred, masked_y_true)\n",
    "        else:\n",
    "            loss = self.zero\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
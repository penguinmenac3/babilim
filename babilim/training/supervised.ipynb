{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# babilim.training.supervised\n",
    "\n",
    "> A trainer for supervised approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Iterable, Union\n",
    "from babilim.training.trainer import Trainer\n",
    "from babilim.data import Dataloader\n",
    "from babilim.core import GradientTape\n",
    "from babilim.core.logging import error\n",
    "from babilim.model.module import Module\n",
    "from babilim.training.callbacks.checkpoint_callback import CheckpointCallback\n",
    "from babilim.training.callbacks.log_callback import LogCallback\n",
    "from babilim.training.callbacks.tensorboard_callback import TensorboardCallback\n",
    "from babilim.training.losses import Loss\n",
    "from babilim.training.optimizers import Optimizer\n",
    "from babilim.training.callbacks.base_callback import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Callbacks\n",
    "\n",
    "The supervised trainer has default callbacks set, unless you specify callbacks yourself.\n",
    "\n",
    "```python\n",
    "DEFAULT_CALLBACKS = [\n",
    "    LogCallback(),\n",
    "    CheckpointCallback(),\n",
    "    TensorboardCallback()\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "DEFAULT_CALLBACKS = [\n",
    "    LogCallback(),\n",
    "    CheckpointCallback(),\n",
    "    TensorboardCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SupervisedTrainer(Trainer):\n",
    "    def __init__(self, model: Module, loss: Loss, optimizer: Optimizer, callbacks: Iterable[BaseCallback] = DEFAULT_CALLBACKS):\n",
    "        \"\"\"\n",
    "        Create a trainer for supervised training scenarios.\n",
    "\n",
    "        The fit function is very basic and can be vastly extended by using callbacks.\n",
    "        The default behaviour can be changed by changing not passing the DEFAULT_CALLBACKS but a modified set of callbacks (only do this if you know what you are doing).\n",
    "        A normal use case would be to simply add some callbacks:\n",
    "            SupervisedTrainer(callbacks=DEFAULT_CALLBACKS + [my_callback])\n",
    "\n",
    "        :param model: The model that should be fit.\n",
    "        :param loss: The loss defines a what should optimization.\n",
    "        :param optimizer: The optimizer defines how the optimization is done.\n",
    "        :param callbacks: Any callbacks that you want to add. You should always write callbacks=DEFAULT_CALLBACKS+[MyCallback], otherwise the default callbacks will not be called.\n",
    "        Callbacks will be called in the order as specified in this list. So make sure your callbacks are in the correct order (and when in doubt DEFAULT_CALLBACKS first, yours later).\n",
    "        \"\"\"\n",
    "        self.callbacks = callbacks\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def run_epoch(self, dataloader: Dataloader, phase: str, epoch: int):\n",
    "        \"\"\"\n",
    "        Run an epoch in training or validation.\n",
    "\n",
    "        (This function is called in fit and it is NOT RECOMMENDED to use this function from outside.)\n",
    "\n",
    "        Optimizer is \"optional\" if it is set to None, it is a validation run otherwise it is a training run.\n",
    "\n",
    "        :param dataloader: The dataloader created from a dataset.\n",
    "        :param phase: The phase (train/dev/test) which is used for running.\n",
    "        :param epoch: The epoch number.\n",
    "        :return: Returns the average loss.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"You must compile the trainer first!\")\n",
    "        self.loss.reset_avg()\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_epoch_begin(dataloader, phase, epoch)\n",
    "\n",
    "        # Setup the training loop\n",
    "        variables = self.model.trainable_variables + self.loss.trainable_variables\n",
    "\n",
    "        # Loop over the dataset_class and update weights.\n",
    "        for iter, (x, y) in enumerate(dataloader):\n",
    "            for callback in self.callbacks:\n",
    "                callback.on_iter_begin(iter, x, y)\n",
    "\n",
    "            # Forward pass, computing gradients and applying them\n",
    "            with GradientTape(variables) as tape:\n",
    "                predictions = self.model(**x._asdict())\n",
    "                for name, p in predictions._asdict().items():\n",
    "                    if p.is_nan().any():\n",
    "                        error(\"NaN NetworkOutput {}: {}\".format(name, p.native))\n",
    "                        raise ValueError(\"NetworkOutput {} got nan.\".format(name))\n",
    "                loss_result = self.loss(y_true=y, y_pred=predictions)\n",
    "                self.loss.log(\"loss/total\", loss_result)\n",
    "                if loss_result.is_nan().any():\n",
    "                    error(\"NaN Loss\")\n",
    "                    raise ValueError(\"Loss got nan.\")\n",
    "            gradients = tape.gradient(loss_result)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                self.optimizer.apply_gradients(gradients, variables)\n",
    "\n",
    "            for callback in self.callbacks:\n",
    "                callback.on_iter_end(predictions, loss_result)\n",
    "\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_epoch_end()\n",
    "\n",
    "    def fit(self, train_dataloader: Dataloader, dev_dataloader: Dataloader, epochs: int):\n",
    "        \"\"\"\n",
    "        Fit the model managed by this trainer to the data.\n",
    "\n",
    "        :param train_dataloader: The dataloader for training your neural network (train split).\n",
    "        :param dev_dataloader: The dataloader for validation during your development (dev split). NOT TEST SPLIT!\n",
    "        :param epochs: The number of epochs describes how often the fit will iterate over the dataloaders.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start_epoch = 0\n",
    "            for callback in self.callbacks:\n",
    "                start_epoch = callback.on_fit_start(self.model, train_dataloader, dev_dataloader, self.loss, self.optimizer, start_epoch, epochs)\n",
    "\n",
    "            for epoch in range(start_epoch, epochs):\n",
    "                self.model.train()\n",
    "                self.run_epoch(train_dataloader, \"train\", epoch)\n",
    "                self.model.eval()\n",
    "                self.run_epoch(dev_dataloader, \"dev\", epoch)\n",
    "        except KeyboardInterrupt as e:\n",
    "            for callback in self.callbacks:\n",
    "                callback.on_fit_interruted(e)\n",
    "        except Exception as e:\n",
    "            for callback in self.callbacks:\n",
    "                callback.on_fit_failed(e)\n",
    "            raise e\n",
    "\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_fit_end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

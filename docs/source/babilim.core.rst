Core
====

The core module contains all core features of babilim.
This include Tensor, GradientTape, RunOnlyOnce-Annotation and StatefullObject.

A **Tensor** is the basic building block of every deep learning framework.
Babilim tensor works very similar to pytorch and tf2 tensors and you will need almost no adaption.

**StatefullObjects** are objects which contain weight tensors and computations.
The Model, Optimizer, Losses and even Metrics are StatefullObjects.
StatefullObjects allow saving their state and reloading it from a checkpoint.

The **GradientTape** is records gradients in a block it surrounds.
It is typically used in a training loop as in this example:

.. code-block:: python

    # Extract from IModel::train_epoch
    with GradientTape(variables) as tape:
        prediction = self(**x._asdict())
        loss_results = loss(y_true=y, y_pred=prediction)
        loss.log("total", loss_results)
        metrics(y_true=y, y_pred=prediction)
    # Translate those to something usefull...
    gradients = tape.gradient(loss_results)


The **RunOnlyOnce** annotation is a powerfull annotation to write build methods or any function that you want to call multiple times but only execute once.
A typical example is in a model, where the build function creates the variables and the call is the forward pass.

.. code-block:: python

    @RunOnlyOnce
    def build(self, features: ITensor):
        # TODO create your variables in here.
        # This method is only executed once thanks to @RunOnlyOnce
        pass

    def call(self, features: ITensor) -> NetworkOutput:
        net = features
        # TODO put your network forward pass here
        return NetworkOutput(class_id=net)

**Autogenerated Documentation**

.. automodule:: babilim.core
   :members:
   :undoc-members:
   :show-inheritance:

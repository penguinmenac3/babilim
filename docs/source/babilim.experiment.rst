Experiment Configuration
========================

This package is about your experiment and data configuration.
DeepLearning has a lot of hyperparameters, your experiment configuration is one central place where all of them should live.
This will ensure that you do not miss out on any of them.

Every configuration must define the following, to allow for `IModel::fit` to work:

.. code-block:: python

    import babilim.optimizers.learning_rates as lr
    from babilim.experiment import Config

    class MyConfig(Config):
    def __init__(self):
        super().__init__()
        # Define your problem
        self.problem_base_dir = "$HOME/datasets"  # use env variables

        # Define how to train your model
        self.train_epochs = 20
        self.train_l2_weight = 0.01
        self.train_batch_size = 32
        self.train_log_steps = 100
        self.train_experiment_name = "Next_NeurIPS_Paper"
        self.train_checkpoint_path = "/trash/checkpoints"  # absolute or relative paths
        self.train_learning_rate_shedule = lr.Exponential(initial_lr=0.001, k=5e-5)

        # Define your architecture parameters
        self.arch_use_fancy_stuff = True  # Not actually required...


As demonstrated with `arch_use_fancy_stuff` you can add any parameters you want to your config.
Also keep in mind, that your config is python code, so you can use inheritance (make use of it intelligently).

Even though dynamic importing of configs is supported with **import_config**, you should NOT use it due to missing type safety on dynamic import.


**Autogenerated Documentation**

.. automodule:: babilim.experiment
   :members:
   :undoc-members:
   :show-inheritance:

Losses & Metrics
================

**Loss**

The loss ties your network output and the dataset labels together.
It is a function that expects two inputs, one for the network outputs y_pred and one for the dataset labels y_true.

There are pre implemented loss functions which work on tensors, but for your problem you will need to define your own custom loss.
However, your custom loss can internally use existing losses.
This is because the network output and dataset output is typed and generic losses do not have those types.
The reason behind that is simply a level of type safety for you.

The simple example below shows how a classification loss could look for your problem:

.. code-block:: python

    from babilim.core import ITensor
    from babilim.losses import Loss, SparseCrossEntropyLossFromLogits

    class MyLoss(Loss):
        def __init__(self):
            super().__init__()
            self.ce = SparseCrossEntropyLossFromLogits()

        def call(self, y_pred: NetworkOutput, y_true: NetworkOutput) -> ITensor:
            return self.ce(y_pred.class_id, y_true.class_id).mean()


**Metrics**

Metrics are similar to a loss, however, they do not return any tensor for optimization and have the sole purpose of giving you insight in your network.

A simple example could be the following:

.. code-block:: python

    from babilim.losses import Metrics, SparseCrossEntropyLossFromLogits, SparseCategoricalAccuracy

    class FashionMnistMetrics(Metrics):
        def __init__(self):
            super().__init__()
            self.ce = SparseCrossEntropyLossFromLogits()
            self.ca = SparseCategoricalAccuracy()

        def call(self, y_pred: NetworkOutput, y_true: NetworkOutput) -> None:
            self.log("ce", self.ce(y_pred.class_id, y_true.class_id).mean())
            self.log("ca", self.ca(y_pred.class_id, y_true.class_id).mean())


**Autogenerated Documentation**

.. automodule:: babilim.losses
   :members:
   :undoc-members:
   :show-inheritance:

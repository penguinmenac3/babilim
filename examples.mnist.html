

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>MNIST Example (Babilim) &mdash; babilim 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="MNIST Example (Pytorch Native)" href="examples.mnist.pytorch.html" />
    <link rel="prev" title="Utils" href="babilim.utils.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> babilim
          

          
          </a>

          
            
            
              <div class="version">
                0.0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials.install.html">Install Babilim</a></li>
</ul>
<p class="caption"><span class="caption-text">Packages</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="babilim.html">Main Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="babilim.callbacks.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="babilim.core.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="babilim.data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="babilim.experiment.html">Experiment Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="babilim.layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="babilim.logger.html">Logging &amp; Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="babilim.losses.html">Losses &amp; Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="babilim.models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="babilim.optimizers.html">Optimizers &amp; Learning Rates</a></li>
<li class="toctree-l1"><a class="reference internal" href="babilim.utils.html">Utils</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">MNIST Example (Babilim)</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.mnist.pytorch.html">MNIST Example (Pytorch Native)</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">babilim</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>MNIST Example (Babilim)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/examples.mnist.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="mnist-example-babilim">
<h1>MNIST Example (Babilim)<a class="headerlink" href="#mnist-example-babilim" title="Permalink to this headline">¶</a></h1>
<p>This example shows how to implement a simple network solving MNIST with babilim.
The focus is not to create the shortest possible solution, but rather to show how babilim works.
The example is build in a way it can be applied to other problems as well.</p>
<p>The full code can be found <a class="reference external" href="https://github.com/penguinmenac3/babilim/blob/master/examples/fashion_mnist.py">here on github</a>.</p>
<p><strong>1. Define the Problem to solve</strong></p>
<p>Even before you start coding anything, you should start to define the problem you want to solve on a piece of paper.
This will help your code quality a lot.</p>
<p>In this tutorial we will solve FashionMnist, classification of 28x28 Pixel fashion images.</p>
<p><strong>2. Define your Inputs and Outputs</strong></p>
<p>With a goal established we can define inputs and outputs to our network.
For that we create a named tuple for input and output, with features and class_id respectively.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>

<span class="c1"># Create some named tuple for our inputs and outputs so we do not confuse them.</span>
<span class="n">NetworkInput</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;NetworkInput&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;features&quot;</span><span class="p">])</span>
<span class="n">NetworkOutput</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;NetworkOutput&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;class_id&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p><strong>3. Create a configuration</strong></p>
<p>To further define our problem we create a configuration to use for our problem and training.
This configuration is something you should create at the start and change and extend as you go.
For the minimal necessary configuration look at the “Experiment Configuration” section of this documentation.</p>
<p>Since we are solving MNIST we setup our number of categories to 10 (for later use in our network) and define a place where to save our dataset.
Furthermore all training parameters that are required get defined.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">babilim.optimizers.learning_rates</span> <span class="kn">as</span> <span class="nn">lr</span>
<span class="kn">from</span> <span class="nn">babilim.experiment</span> <span class="kn">import</span> <span class="n">Config</span>

<span class="k">class</span> <span class="nc">FashionMnistConfig</span><span class="p">(</span><span class="n">Config</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">problem_number_of_categories</span> <span class="o">=</span> <span class="mi">10</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">problem_samples</span> <span class="o">=</span> <span class="mi">1875</span> <span class="o">*</span> <span class="mi">32</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">problem_base_dir</span> <span class="o">=</span> <span class="s2">&quot;datasets&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train_epochs</span> <span class="o">=</span> <span class="mi">20</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_l2_weight</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">=</span> <span class="mi">32</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_log_steps</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_experiment_name</span> <span class="o">=</span> <span class="s2">&quot;FashionMNIST&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_checkpoint_path</span> <span class="o">=</span> <span class="s2">&quot;checkpoints&quot;</span>
        <span class="n">samples_per_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">problem_samples</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_learning_rate_shedule</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="n">initial_lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">0.1</span> <span class="o">/</span> <span class="n">samples_per_epoch</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>4. Write your dataset loader</strong></p>
<p>Next step is writing your dataset loader.
You should take your time with this step and thoroughly test your implementation, visually and with test cases.
In this tutorial we will not test but just provide an example on how to write one.</p>
<p>Since this tutorial should work for both pytorch and tensorflow we will write a wrapper around the FashionMNIST dataset provided by the frameworks.
This wrapper will show the concept of how to write platform specific parts of the code.</p>
<p>In the __init__ function we start by calling the super. And then handling the tensorflow case.
For that we check if the backend is tensorflow and then load fashion mnist from tensorflow.
Note the import being only in the tensorflow case because it would break for a pytorch environment.
After saving the variables as local variables we implement the pytorch case, where we simply import the pytorch mnist dataset implementation and use it.
The problem_base_dir from the configuration is used to store the dataset.</p>
<p>In the __len__ function we return the number of training samples our dataset has, dependant on it being for training or validation.</p>
<p>The getitem function actually returns the training sample.
Here we read the example from the stored array.
More complex datasets might require loading from the disk here.
Then, we transform the data to have the shape we want, before the image data is wrapped using image_grid_wrap and then used in our named tuple we defined in step 1.
Your dataset should always output two namedtuples one for the features and one for the labels.</p>
<p>At the end we define a version of our dataset.
This is important for larger projects where your dataset might generate caches which you need to check for validity.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">babilim</span>
<span class="kn">from</span> <span class="nn">babilim</span> <span class="kn">import</span> <span class="n">TF_BACKEND</span><span class="p">,</span> <span class="n">PYTORCH_BACKEND</span>
<span class="kn">from</span> <span class="nn">babilim.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">FashionMnistDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">FashionMnistConfig</span><span class="p">,</span> <span class="n">phase</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">babilim</span><span class="o">.</span><span class="n">is_backend</span><span class="p">(</span><span class="n">TF_BACKEND</span><span class="p">):</span>
            <span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">fashion_mnist</span>
            <span class="p">((</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainY</span><span class="p">),</span> <span class="p">(</span><span class="n">valX</span><span class="p">,</span> <span class="n">valY</span><span class="p">))</span> <span class="o">=</span> <span class="n">fashion_mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trainX</span> <span class="o">=</span> <span class="n">trainX</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trainY</span> <span class="o">=</span> <span class="n">trainY</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">valX</span> <span class="o">=</span> <span class="n">valX</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">valY</span> <span class="o">=</span> <span class="n">valY</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">FashionMNIST</span>
            <span class="n">dataset</span> <span class="o">=</span> <span class="n">FashionMNIST</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">problem_base_dir</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">phase</span><span class="o">==</span><span class="n">PHASE_TRAIN</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trainX</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trainY</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trainX</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trainY</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">valX</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainX</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">valY</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainY</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">phase</span> <span class="o">==</span> <span class="n">PHASE_TRAIN</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainX</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valX</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">getitem</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">NetworkInput</span><span class="p">,</span> <span class="n">NetworkOutput</span><span class="p">]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainY</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainX</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valY</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valX</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>

        <span class="n">feat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">feat</span><span class="p">,</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">NetworkInput</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">image_grid_wrap</span><span class="p">(</span><span class="n">feat</span><span class="p">)),</span> <span class="n">NetworkOutput</span><span class="p">(</span><span class="n">class_id</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">version</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;FashionMnistDataset&quot;</span>
</pre></div>
</div>
<p><strong>5. Defining the model</strong></p>
<p>Finally, after we have our dataset working and tested, we can define our model.</p>
<p>A model has three parts to it:</p>
<ul class="simple">
<li><p>the initializer __init__ where the configuration is stored as part of the object,</p></li>
<li><p>the build function which creates the layers</p></li>
<li><p>and the call function which contains the forward pass of the network.</p></li>
</ul>
<p>Here we can see that the init is pretty empty apart from creating the python variables required.
The build function creates all the layers and appends them to our linear list of layers.
In the call function all layers in linear are executed one after another in a for loop.</p>
<p>The inputs of the build and call are the fields of NetworkInputs defined in step 1.
The return value of the call function is of type NetworkOutput.
This makes your network type safe and avoids some unwanted surprises.</p>
<p>Creation of layers works like in keras as it is the simplest way.
Not that in contrast to native pytorch you do not need to define the input shapes of a layer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">babilim.core</span> <span class="kn">import</span> <span class="n">ITensor</span><span class="p">,</span> <span class="n">RunOnlyOnce</span>
<span class="kn">from</span> <span class="nn">babilim.layers</span> <span class="kn">import</span> <span class="n">BatchNormalization</span><span class="p">,</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span><span class="p">,</span> <span class="n">GlobalAveragePooling2D</span><span class="p">,</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">ReLU</span><span class="p">,</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="nn">babilim.models</span> <span class="kn">import</span> <span class="n">IModel</span>

<span class="k">class</span> <span class="nc">FashionMnistModel</span><span class="p">(</span><span class="n">IModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">FashionMnistConfig</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;FashionMnistModel&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">layer_type</span><span class="o">=</span><span class="s2">&quot;FashionMnistModel&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="nd">@RunOnlyOnce</span>
    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">:</span> <span class="n">ITensor</span><span class="p">):</span>
        <span class="n">out_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_number_of_categories</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">())</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">())</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">())</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GlobalAveragePooling2D</span><span class="p">())</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="mi">18</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="n">out_features</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">:</span> <span class="n">ITensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NetworkOutput</span><span class="p">:</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">features</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">:</span>
            <span class="n">net</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">NetworkOutput</span><span class="p">(</span><span class="n">class_id</span><span class="o">=</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>6. Defining the Loss and Metrics</strong></p>
<p>With a model, the last step before training is to setup some losses and metrics.</p>
<p>The loss is pretty simple. It is a class implementing a call function which has two parameters.
The first parameter is y_pred representing the actual network output, and y_true is the intended network output as returned by the dataset.
Intermediate computations such as partial losses can be logged using self.log(name, tensor).
The return type of the loss is a single Tensor, the loss that should be optimized.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">babilim.core</span> <span class="kn">import</span> <span class="n">ITensor</span>
<span class="kn">from</span> <span class="nn">babilim.losses</span> <span class="kn">import</span> <span class="n">Loss</span><span class="p">,</span> <span class="n">SparseCrossEntropyLossFromLogits</span>

<span class="k">class</span> <span class="nc">FashionMnistLoss</span><span class="p">(</span><span class="n">Loss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ce</span> <span class="o">=</span> <span class="n">SparseCrossEntropyLossFromLogits</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">:</span> <span class="n">NetworkOutput</span><span class="p">,</span> <span class="n">y_true</span><span class="p">:</span> <span class="n">NetworkOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ITensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ce</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">class_id</span><span class="p">,</span> <span class="n">y_true</span><span class="o">.</span><span class="n">class_id</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<p>The metric is very similar to the loss. It is a class implementing a call function which has two parameters.
The first parameter is y_pred representing the actual network output, and y_true is the intended network output as returned by the dataset.
However, it does not have a return type and no effect on the optimization.
Whereas it has no return type values must be explicitly logged using self.log(name, tensor).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">babilim.losses</span> <span class="kn">import</span> <span class="n">Metrics</span><span class="p">,</span> <span class="n">SparseCrossEntropyLossFromLogits</span><span class="p">,</span> <span class="n">SparseCategoricalAccuracy</span>

<span class="k">class</span> <span class="nc">FashionMnistMetrics</span><span class="p">(</span><span class="n">Metrics</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ce</span> <span class="o">=</span> <span class="n">SparseCrossEntropyLossFromLogits</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ca</span> <span class="o">=</span> <span class="n">SparseCategoricalAccuracy</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">:</span> <span class="n">NetworkOutput</span><span class="p">,</span> <span class="n">y_true</span><span class="p">:</span> <span class="n">NetworkOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;ce&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ce</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">class_id</span><span class="p">,</span> <span class="n">y_true</span><span class="o">.</span><span class="n">class_id</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;ca&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ca</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">class_id</span><span class="p">,</span> <span class="n">y_true</span><span class="o">.</span><span class="n">class_id</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<p><strong>7. Training it</strong></p>
<p>Finally we can write code which glues everything together.
First select your backend of choice.
Then, create a configuration and use it to setup the logger module.</p>
<p>After that you can create your dataset for training and validation by instantiating the class created in step 4.
Also our model from step 5 can be instantiated as well as the loss and metrics from step 6.
Finally we select an optimizer (typically SGD is fine).</p>
<p>With all objects instantiated, we can call the fit method on the model to actually fit the model against the data using our configuration, optimizer and learning rate schedule.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">babilim</span>
<span class="kn">import</span> <span class="nn">babilim.logger</span> <span class="kn">as</span> <span class="nn">logger</span>
<span class="kn">from</span> <span class="nn">babilim.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">babilim</span> <span class="kn">import</span> <span class="n">PYTORCH_BACKEND</span><span class="p">,</span> <span class="n">TF_BACKEND</span><span class="p">,</span> <span class="n">PHASE_TRAIN</span><span class="p">,</span> <span class="n">PHASE_VALIDATION</span>

<span class="n">babilim</span><span class="o">.</span><span class="n">set_backend</span><span class="p">(</span><span class="n">PYTORCH_BACKEND</span><span class="p">)</span>

<span class="c1"># Create our configuration (containing all hyperparameters)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">FashionMnistConfig</span><span class="p">()</span>
<span class="n">logger</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">continue_training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Load the data</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">FashionMnistDataset</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PHASE_TRAIN</span><span class="p">)</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">FashionMnistDataset</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PHASE_VALIDATION</span><span class="p">)</span>

<span class="c1"># Create a model.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FashionMnistModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Create a loss and some metrics (if your loss has hyperparameters use config for that)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">FashionMnistLoss</span><span class="p">()</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">FashionMnistMetrics</span><span class="p">()</span>

<span class="c1"># Create optimizer</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">()</span>

<span class="c1"># Fit our model to the data using our loss and report the metrics.</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">train_learning_rate_shedule</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>8. What next?</strong></p>
<p>Solve your own problem in a similar manner.
Dive into the detailed api documentation and even have peeks at the code to become a true master in using babilim.</p>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="examples.mnist.pytorch.html" class="btn btn-neutral float-right" title="MNIST Example (Pytorch Native)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="babilim.utils.html" class="btn btn-neutral float-left" title="Utils" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Michael Fuerst

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>